{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "m2p_C03_An Efficient Preconditioner for Stochastic Gradient Descent Optimization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNeCW7RgnbkQqnSo/XIxCpD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amey-joshi/am/blob/master/p4/m2p_C03_An_Efficient_Preconditioner_for_Stochastic_Gradient_Descent_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-ROn6n0X9vJ",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "This tutorial is based on the paper [An Efficient Preconditioner for Stochastic\n",
        "Gradient Descent Optimization of Image Registration](https://drive.google.com/drive/folders/1uGhRukl6ZBdZhOOhfqF1K3DzLT3xN-F_) by Yuchuan Qiao , Boudewijn P. F. Lelieveldt, and Marius Staring. It assumes that the reader has gone through the earlier tutorial on adaptive stochastic gradient algorithm for image registration. This tutorial will reuse the algorithm developed in that tutorial and will just plug in the pre-conditioner in the same algorithm.\n",
        "\n",
        "We briefly explained the various methods to find extrema of functions in our previous tutorials on adaptive stochastic gradient method and stochastic quasi-Newton methods. The image registration algorithm is an optimization algorithm described as\n",
        "$$\\tag{1}\n",
        "\\hat{\\boldsymbol{\\mu}} = \\mathrm{argmin}_\\boldsymbol{\\mu}\\;\\mathcal{C}(I_F, T_\\boldsymbol{\\mu}(I_M)),\n",
        "$$\n",
        "where\n",
        "\n",
        "\n",
        "*   $I_F$ is the fixed image;\n",
        "*   $I_M$ is the moving image;\n",
        "*   $T_\\boldsymbol{\\mu}$ is a parametric transformation taking the moving image to the fxed image. $\\boldsymbol{\\mu}$ are the parameters of the transformation.\n",
        "*   $\\mathcal{C}$ is the cost function. It is a measure of the dissimilarity between its arguments.\n",
        "\n",
        "The optimization consists in finding $\\boldsymbol{\\mu}$ that minimizes the cost function, that is minimizes the dissimilarity between the moving and the fixed images. A typical gradient descent algorithm is expressed as\n",
        "$$\\tag{2}\n",
        "\\boldsymbol{\\mu}_{k+1} = \\boldsymbol{\\mu} - \\gamma_k \\boldsymbol{D}_k,\n",
        "$$\n",
        "where $\\boldsymbol{D}_k$ is either the gradient of $\\mathcal{C}$ with respect to $\\boldsymbol{\\mu}$ or a closely related function. For the purpose of this discussion it is appropriate to let $\\boldsymbol{D}_k$ be $\\nabla\\mathcal{C}_{\\boldsymbol{\\mu}_k}$, the gradient of $\\mathcal{C}$ computed at the $k$-th step. In the next section we explain the idea of a preconditioner.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydl9JFNka7lN",
        "colab_type": "text"
      },
      "source": [
        "# The preconditioner\n",
        "In principle equations (1) and (2) are a perfect definition of an optimization problem. In practice, they fall short if the cost function is ill-conditioned leading to numerical instability. Let us therefore understand what being ill-conditioned means in the context of gradient descent. We know that the gradient descent algorithm finds the direction of the steepest gradient and chooses to descend in the exactly opposite direction. Often times, the function is of the shape of a long narrow valley. If one descends along the \"long direction\" then one can hit the minimum only after a large number of iterations. In many of these iterations the function changes very little and some optimizers might stop believing that they have found a minimum. On the other hand, if one descends down perpendicular to the \"long direction\", one will see the value of the function dropping drastically. An adaptive gradient descent algorithm chooses a large 'step size' in the former case in which the function changes gently. It chooses a small step size in the latter case when the value of the function drops very fast. While this idea works well in many situations, it does break when in extreme situations when the valley is very deep and long. In such situations it is beneficial to transform our function so that the descent in all directions is roughly the same.\n",
        "\n",
        "We recommend going over the lectures in Cornell university's course [Principles of Large-Scale Machine Learning â€” Spring 2019](https://www.cs.cornell.edu/courses/cs4787/2019sp/) for a deeper understanding of the idea of a condition number of a problem and the preconditioner. In particular, we recommend [lecture 4](https://www.cs.cornell.edu/courses/cs4787/2019sp/notes/lecture4.pdf) and [lecture 7](https://www.cs.cornell.edu/courses/cs4787/2019sp/notes/lecture7.pdf).\n",
        "\n",
        "With a pre-conditioner, equation (2) becomes\n",
        "$$\\tag{3}\n",
        "\\boldsymbol{\\mu}_{k+1} = \\boldsymbol{\\mu} - \\gamma_k \\boldsymbol{P} \\boldsymbol{D}_k,\n",
        "$$\n",
        "where $\\boldsymbol{P}$ is the preconditioner matrix. It can either be the same for all iterations or it may be recomputed for each iteration. In the latter case, one might choose to denote it by $\\boldsymbol{P}_k$. If $\\boldsymbol{P} = \\boldsymbol{I}$, the identity matrix, then the we have the normal stochastic gradient algorithm. It is especially important for an image processing application to have $\\boldsymbol{P}$ which is easily computed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXIjDpNufOto",
        "colab_type": "text"
      },
      "source": [
        "# The choice of pre-conditioner\n",
        "The gradient descent algorithm computes a different set of transformation parameters in each iteration. A change in the parameters changes the transformation by a certain amount. A voxel will be moved to one point $T_{\\boldsymbol{\\mu}_k}$ and another point by $T_{\\boldsymbol{\\mu}_{k+1}}$. We denote the incremental displacement in the second transformation by $d_k$. Note that $d_k$ will not be the same for the entire image. Rather, it is a function of the voxel. Thus,\n",
        "$$\\tag{4}\n",
        "d_k(x_j) = T_{\\boldsymbol{\\mu}_{k+1}}(x_j) - T_{\\boldsymbol{\\mu}_{k}}(x_j).\n",
        "$$\n",
        "If the parameters do not change drastically across iterations, we can approximate the above equation as\n",
        "$$\\tag{5}\n",
        "d_k(x_j) = \\frac{\\partial{T_{\\boldsymbol{\\mu}_{k}}(x_j)}}{{\\partial\\boldsymbol{\\mu}_{k}}}(\\boldsymbol{\\mu}_{k+1} - \\boldsymbol{\\mu}_{k}).\n",
        "$$\n",
        "If the image is $d$-dimensional and if the number of parameters is $N_p$ then \n",
        "$$\\tag{6}\n",
        "\\boldsymbol{J}(x_j) = \\frac{\\partial{T_{\\boldsymbol{\\mu}_{k}}(x_j)}}{{\\partial\\boldsymbol{\\mu}_{k}}}\n",
        "$$\n",
        "is a $d \\times N_p$ matrix. The authors call $J$ the jacobian matrix. However, we will just call it the $J$-matrix. Using equations (3) and (6) in (5) we get\n",
        "$$\\tag{7}\n",
        "d_k(x_j) = -\\gamma_k \\boldsymbol{J}(x_j) \\boldsymbol{P}\\boldsymbol{D}_k \n",
        "$$\n",
        "This paper approximates the preconditioner $\\boldsymbol{P}$ by a diagonal matrix so that the product $\\boldsymbol{P}\\boldsymbol{D}_k$ is very easy to compute. The diagnal elements are\n",
        "$$\\tag{8}\n",
        "p_i = \\frac{\\delta}{\\mathrm{E}(s)i) + 2\\sqrt{\\mathrm{Var}(s_i)} + \\varepsilon},\n",
        "$$\n",
        "where $\\delta$ and $\\varepsilon$ are constants and $s_i = ||\\boldsymbol{J}(x_j)||.||\\boldsymbol{D}_k||$, $$||\\cdot||$ being the $L^2$ norm of a column of the matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6CdjxJenXD1",
        "colab_type": "text"
      },
      "source": [
        "# Implementation\n",
        "We implement the preconditioner in functions <code>compute_P</code> and <code>compute_q</code> in the code snippet in the cells below. We continue to work with the same binary images we have been using all along and we try the linear transformation $T(x) = \\mu_0 + \\mu_1 x$ on the image. This code is the same as we used in the tutorial on adaptive stochastic gradient except for the addition of two new functions to compute the preconditioner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz33AbU5oDPo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "b79057e5-a3bd-48dc-edc4-ed537b23f354"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import random\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "f = np.zeros(36)\n",
        "f[14] = f[15] = f[20] = f[21] =  f[35] = 128\n",
        "ref = Image.new('1', (6, 6))\n",
        "ref.putdata(f)\n",
        "\n",
        "m = np.zeros(36)\n",
        "m[7] = m[8] = m[13] = m[14] = m[28] = 128\n",
        "tst = Image.new('1', (6, 6))\n",
        "tst.putdata(m)\n",
        "\n",
        "plt.figure()\n",
        "plt.suptitle('Fixed and moving images')\n",
        "p1 = plt.subplot(121)\n",
        "p1.set_title('Fixed')\n",
        "plt.imshow(ref)\n",
        "\n",
        "p2 = plt.subplot(122)\n",
        "p2.set_title('Moving')\n",
        "plt.imshow(tst)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "x = f - m # This is the \"error\" between the images.\n",
        "C = np.array([[36, np.sum(x)], [np.sum(x), np.sum(x**2)]])/36\n",
        "\n",
        "DELTA = 0.1\n",
        "den_sigma_4 = np.zeros(len(x))\n",
        "delta_sq = DELTA**2\n",
        "\n",
        "for j in range(0, len(den_sigma_4)):\n",
        "  a = 1 + x[j]**2\n",
        "  den_sigma_4[j] = delta_sq/a\n",
        "\n",
        "sigma_4_sq = np.min(den_sigma_4)/(1 + 2 * np.sqrt(2))\n",
        "sigma_4 = np.sqrt(sigma_4_sq)\n",
        "\n",
        "# This is our starting vector\n",
        "mu_0 = np.array([0, 0])\n",
        "cov = np.array([[sigma_4_sq, 0], [0, sigma_4_sq]])\n",
        "mu_samples = []\n",
        "N = 20 # number of samples\n",
        "\n",
        "frozen = stats.multivariate_normal(mu_0, cov)\n",
        "mu_samples = frozen.rvs(10)\n",
        "\n",
        "random.seed(12111842)\n",
        "\n",
        "exact_g = []\n",
        "apprx_g = []\n",
        "error_g = []\n",
        "\n",
        "for i in range(len(mu_samples)):\n",
        "  mu = mu_samples[i]\n",
        "  g = np.zeros(2)\n",
        "  for j in range(len(x)):\n",
        "    # Recall that the data for the fixed image is in the array x\n",
        "    X = np.array([1, x[j]]) \n",
        "    coeff = -2 * ((1 - mu[1]) * x[j] - mu[0])\n",
        "    g = g + coeff * X\n",
        "\n",
        "  exact_g.append(g)\n",
        "\n",
        "for i in range(len(mu_samples)):\n",
        "  mu = mu_samples[i]\n",
        "  g = np.zeros(2)\n",
        "  # Select a new set of pixels for computation of each approximate gradient\n",
        "  S = random.sample(range(0, 36), N)\n",
        "  for j in range(len(S)):\n",
        "    # Recall that the data for the fixed image is in the array x\n",
        "    X = np.array([1, x[j]]) \n",
        "    coeff = -2 * ((1 - mu[1]) * x[j] - mu[0])\n",
        "    g = g + coeff * X\n",
        "\n",
        "  apprx_g.append(g)\n",
        "\n",
        "for i in range(len(mu_samples)):\n",
        "  error_g.append(exact_g[i] - apprx_g[i])\n",
        "  \n",
        "trace_C = C[0, 0] + C[1, 1]\n",
        "\n",
        "s = 0 \n",
        "for i in range(len(exact_g)):\n",
        "  g_n = exact_g[i]\n",
        "  s = s + g_n[0]**2 + g_n[1]**2\n",
        "\n",
        "sigma_1_sq = 1/(N * trace_C)\n",
        "\n",
        "all_a = np.zeros(len(x))\n",
        "sigma_1 = np.sqrt(sigma_1_sq)\n",
        "coeff = 0.1 / sigma_1\n",
        "\n",
        "for i in range(len(x)):\n",
        "  a = (1 + 2 * np.sqrt(2)) * (1 + 3.64 * 1E3 * x[i]**4)\n",
        "  all_a[i] = coeff/np.sqrt(a)\n",
        "\n",
        "a_max = np.min(all_a)\n",
        "\n",
        "s1 = 0\n",
        "s2 = 0\n",
        "for i in range(len(exact_g)):\n",
        "  g = exact_g[i]\n",
        "  h = apprx_g[i]\n",
        "\n",
        "  s1 = s1 + (g[0]**2 + g[1]**2)\n",
        "  s2 = s2 + (h[0]**2 + h[1]**2)\n",
        "\n",
        "eta = s1/(s1 + s2)\n",
        "a = a_max * eta\n",
        "f_min = eta - 1\n",
        "\n",
        "err_prod = np.zeros(len(error_g) - 1)\n",
        "zeta = 0.1\n",
        "\n",
        "for i in range(1, len(error_g)):\n",
        "  e1 = error_g[i]\n",
        "  e2 = error_g[i - 1]\n",
        "  err_prod[i - 1] = np.dot(e1, e2)\n",
        "\n",
        "omega = zeta * np.std(err_prod)\n",
        "\n",
        "def our_sigmoid(x):\n",
        "  # Recall that f_max = 1\n",
        "  return f_min + (1 - f_min)/(1 - np.exp(-x/omega)/f_min)\n",
        "\n",
        "def calc_t(prev_t, g_curr, g_prev):\n",
        "  arg = prev_t + our_sigmoid(np.dot(g_curr, g_prev))\n",
        "  if arg > 0:\n",
        "    return arg\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "# We re-initialize the image data\n",
        "f = np.zeros(36)\n",
        "f[14] = f[15] = f[20] = f[21] =  f[35] = 128\n",
        "m = np.zeros(36)\n",
        "m[7] = m[8] = m[13] = m[14] = m[28] = 128\n",
        "# 'Difference' between fixed and moving images.\n",
        "x = f - m\n",
        "\n",
        "# Parameters of the algorithm\n",
        "MAX_ITER = 300 # Maximum number of iterations.\n",
        "n_iter = 0     # Current iteration.\n",
        "\n",
        "# Temporary variables\n",
        "g_prev = np.array([1, 1]) # previous gradient\n",
        "mu_prev = np.zeros(2)     # previous parameters\n",
        "prev_t = 0                # previous 'time'\n",
        "\n",
        "def compute_q(s):\n",
        "    x = np.sum(s)/len(s)\n",
        "    y = np.sum(s**2)/len(s)\n",
        "    \n",
        "    return x + 2 * np.sqrt(np.abs(y - x**2))\n",
        "\n",
        "def compute_P(s1, s2):\n",
        "    q1 = compute_q(s1)\n",
        "    q2 = compute_q(s2)\n",
        "    \n",
        "    p1 = 1/(q1 + 0.01)\n",
        "    p2 = 1/(q2 + 0.01)\n",
        "    \n",
        "    return np.diag([p1, p2])\n",
        "\n",
        "while n_iter < MAX_ITER:\n",
        "  n_iter = n_iter + 1\n",
        "  m = mu_prev[0] + mu_prev[1] * f\n",
        "  \n",
        "  # Select a new set of pixels for computation of each approximate gradient\n",
        "  S = random.sample(range(0, 36), 15)\n",
        "  g = np.zeros(2)\n",
        "  \n",
        "  s1 = 0\n",
        "  s2 = 0\n",
        "  for j in range(len(S)):\n",
        "    # Recall that the data for the fixed image is in the array x\n",
        "    X = np.array([1, x[j]]) \n",
        "    coeff = -2 * ((1 - mu[1]) * x[j] - mu[0])    \n",
        "    g = g + coeff * X    \n",
        "    \n",
        "  s1 = np.zeros(len(S))\n",
        "  s2 = np.zeros(len(S))\n",
        "  for j in range(len(S)):\n",
        "      s1[j] = 1 * g[0]\n",
        "      s2[j] = x[j] * g[1]\n",
        "  \n",
        "  curr_t = calc_t(prev_t, g, g_prev)  \n",
        "  gamma = a_max/(curr_t + 1) # Recall that A = 1, alpha = 1\n",
        "  \n",
        "  # Compute the pre-conditioner\n",
        "  P = compute_P(s1, s2)\n",
        "\n",
        "  mu_prev = mu_prev + gamma * np.dot(P, g)\n",
        "  g_prev = g\n",
        "  prev_t = curr_t\n",
        "  \n",
        "print(f'mu = {mu_prev}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "mu = [ 8.76669048e-05 -6.84906639e-07]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}