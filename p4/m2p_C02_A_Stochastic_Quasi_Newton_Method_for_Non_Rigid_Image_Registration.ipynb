{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "m2p_C02_A Stochastic Quasi-Newton Method for Non-Rigid Image Registration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCD1WvzeZpLDxqJVA/wY/W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amey-joshi/am/blob/master/p4/m2p_C02_A_Stochastic_Quasi_Newton_Method_for_Non_Rigid_Image_Registration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC7blCvwPwcZ",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "This tutorial is based on the paper [A Stochastic Quasi-Newton Method for Non-rigid Image Registration](https://drive.google.com/drive/folders/1iNk4CD8wlBt0reTZBk6NPLeseIT9EXIV) by Yuchuan Qiao, Zhuo Sun, Boudewijn P.F. Lelieveldt, and Marius Staring. It assumes that the reader has gone over our previous tutorials on image registration. Therefore, it omits an exhaustive discussion of the idea of image registration and basic optimization techniques.\n",
        "\n",
        "Briefly, image registration involves aligning two images of the same object taken at 1) different orientations, 2) different times or 3) using different sensors. If $I_F$ is a **fixed image**, $I_M$ is the **moving image** and $T_{\\boldsymbol{\\mu}}$ is a transformation with parameter vector $\\boldsymbol{\\mu}$ such that $T_{\\boldsymbol{\\mu}}I_M$ is an approximation of $I_F$ then the image registration problem is to find the **best** such $\\boldsymbol{\\mu}$. The idea of best is expressed in mathematical terms as the minimum of a certain **cost function** $\\mathcal{C}$ that describes the **dissimilarity** between $I_F$ and $T_{\\boldsymbol{\\mu}}(I_M)$. If $T_{\\boldsymbol{\\mu}}(I_M)$ is a good approximation to $I_F$ then the cost $\\mathcal{C}(I_F, T_\\boldsymbol{\\mu}(I_M))$ is indeed a minimum. We can express this mathematically as\n",
        "$$\\tag{1}\n",
        "\\hat{\\boldsymbol{\\mu}} = \\mathrm{argmin}_\\boldsymbol{\\mu}\\;\\mathcal{C}(I_F, T_\\boldsymbol{\\mu}(I_M)).\n",
        "$$\n",
        "Thus, the image registration problem is at its heart an optimization problem. In the next section we give a brief sketch of the optimization methods that eventually lead us to the stochastic least memory  Broyden–Fletcher–Goldfarb–Shanno (s-LBFGS) method used in this paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNF5ilyeVRAW",
        "colab_type": "text"
      },
      "source": [
        "# Methods\n",
        "A function has its extremum when its derivative, or gradient if it is a multi-variable function, is zero. Finding an extremum is thus equivalent to finding the root of the derivative (or gradient) function. That is the reason many root finding algorithms find their use in optimization algorithms. Perhaps the oldest algorithm to find roots it the **Newton's method**. If $f$ is a function of $x$ then to find an extremum of $f$ we must find an $x_0$ such that $f^\\prime(x_0) = 0$. Thus the problem of finding an extremum of $f$ is identical to finding the root of its derivative. The Taylor expansion of $f$, up to the quadratic terms, is\n",
        "$$\\tag{2}\n",
        "f(x_k + h) \\approx f(x_k) + hf^\\prime(x_k) + \\frac{h^2}{2}f^{\\prime\\prime}(x_k).\n",
        "$$\n",
        "The approximate form will have a minimum at \n",
        "$$\\tag{3}\n",
        "h = -\\frac{f^\\prime(x_k)}{f^{\\prime\\prime}(x_k)}.\n",
        "$$\n",
        "Therefore, we can update our root to\n",
        "$$\\tag{4}\n",
        "x_{k+1} = x_k + h = x_k -\\frac{f^\\prime(x_k)}{f^{\\prime\\prime}(x_k)}.\n",
        "$$\n",
        "When we reach the extremum, $x_{k+1} = x_k$. This idea is generalized to the case when $f$ is a function of more than one variables by replacing $f^\\prime(x_k)$ with the gradient $\\nabla f(x_k)$ and the reciprocal of the second derivative by the inverse of the Hessian matrix $B_k$. Thus, equation (4) becomes\n",
        "$$\\tag{5}\n",
        "x_{k+1} = x_k - B_k^{-1}\\nabla f(x_k).\n",
        "$$\n",
        "A strict implementation of Newton's method involves finding the inverse of the Hessian matrix at every iteration. If the number of variables on which $f$ depends in large then this is prohibitively expensive. In the case of the cost function of a $512 \\times 512$ image the number of variables will be $n = 2^{18}$. This is where the **quasi-Newton** methods come into picture. In these methods we do not use the actual hessian matrix but an approximation to it. There a large number of choices of the approximate form of the hessian matrix, each one giving rise to a variant of quasi-Newton method. One of the most popular methods is BFGS, named after its inventors Broyden-Fletcher-Goldfarb-\n",
        "Shanno. If $H_k = B_k^{-1}$ then the BFGS approximation is\n",
        "$$\\tag{6}\n",
        "H_{k+1} = V_k^TH_kV_k + \\rho_ks_k s_k^T,\n",
        "$$\n",
        "where\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\rho_k &=& (y_k^T s_k)^{-1} \\tag{7} \\\\\n",
        "V_k &=& I - \\rho_k y_k s_k^T \\tag{8} \\\\\n",
        "s_k &=& \\mu_{k+1} - \\mu_k \\tag{9} \\\\\n",
        "y_k &=& g_{k+1} - g_{k} \\tag{10},\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "$g_k$ being the same as $\\nabla f(x_k)$. If $f$ is a function of $n$ variables then the Hessian matrix and its approximation have size $n \\times n$. For large $n$ storing the Hessian itself will take up a lot of memory. The **limited memory BFGS** algorithm approximates the hessian further by storing only a few vectors that can reconstruct the hessian. Thus the storage demand drops from $O(n^2)$ to $O(n)$. **Python code** implementing these algorithms is made available for academic use by [J. Bayer, C. Osendorfer, S. Diot-Girard, T. Rückstiess and Sebastian Urban. climin - A pythonic framework for gradient-based function optimization. TUM Tech Report. 2016.](https://github.com/BRML/climin/blob/master/climin/bfgs.py)\n",
        "\n",
        "If $g_k = \\nabla f(x_k)$ and $H_k = B_k^{-1}$ then one can write equation (5) as\n",
        "$$\\tag{11}\n",
        "x_{k+1} = x_k - \\gamma_k H_k g_k,\n",
        "$$\n",
        "where we have introduced a 'step size' $\\gamma_k$. We thus observe that we really do not need to store the $n \\times n$ matrix $H_k$ if we can find a way to estimate the $n$-vector $H_k g_k$. One way to estimate the product $H_k g_k$ is the two-step recursion explained in [Wikipedia](https://en.wikipedia.org/wiki/Limited-memory_BFGS)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48CkJ57nm0fZ",
        "colab_type": "text"
      },
      "source": [
        "# Stochastic quasi-Newton method\n",
        "Equations (6) to (10) suggest that all the quantities can be readily computed from the pair $\\{s_k, y_k\\}$, called **curvature points** by the authors. Computing these at each iteration involves all points of the data set. One way to reduce this effort is to work with only a subset of these. For this is how the stochastic gradient descent algorithm works. However the authors believe that the stochastic gradient descent algorithm is inherently noisy and the problem is exacerbated in the case of medical images by the fact that each computation of a 'stochastic' gradient potential involves a different set of voxels.\n",
        "\n",
        "One way to tide over this problem is:\n",
        "\n",
        "*   After every $L$ iterations denoise the optimization parameters $\\boldsymbol{\\mu}$ by averaging them. Use a small random subset $\\mathcal{S}_1$ in each of these iterations.\n",
        "*   Use the average to compute the full Hessian using a random subset $\\mathcal{S}_2$ of the data.\n",
        "\n",
        "The authors adapt this idea to medical images by\n",
        "\n",
        "*   Using more samples in the $L$ iterations used for updating $\\{s_k, y_k\\}$.\n",
        "*   Computing instead of the hessian the difference in gradients. Thus, $y_k = g(\\bar{\\boldsymbol{\\mu}}_I; \\mathcal{S}_2) - g(\\bar{\\boldsymbol{\\mu}}_J; \\mathcal{S}_2)$. Note that the **same** random subset $\\mathcal{S}_2$ of data is used for the computation of both gradients.\n",
        "\n",
        "The step size $\\gamma_k$ in equation (11) is chosen in a manner similar to what we did in the tutorial on adaptive stochastic gradient. That is,\n",
        "$$\\tag{12}\n",
        "\\gamma_k = \\frac{\\eta a}{(t_k + A)^\\alpha},\n",
        "$$\n",
        "where $A \\ge 1, a > 0$ and $\\alpha, \\eta \\in [0, 1]$. We continue the choice of $\\alpha = 1$ made in the tutorial in adaptive stochastic gradient. Refer to the same tutorial for the interpretation of $t_k$ as 'time'. We now present a python function implementing the algorithm. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iHIUUXwA7IK",
        "colab_type": "text"
      },
      "source": [
        "# Implementation\n",
        "We continue to use two binary images to illustrate the idea."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNS0H_aUBBIi",
        "colab_type": "code",
        "outputId": "9c59495a-41b7-4b30-8359-10b6c6f1734d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "f = np.zeros(36)\n",
        "f[14] = f[15] = f[20] = f[21] =  f[35] = 128\n",
        "ref = Image.new('1', (6, 6))\n",
        "ref.putdata(f)\n",
        "\n",
        "m = np.zeros(36)\n",
        "m[7] = m[8] = m[13] = m[14] = m[28] = 128\n",
        "tst = Image.new('1', (6, 6))\n",
        "tst.putdata(m)\n",
        "\n",
        "plt.figure()\n",
        "plt.suptitle('Fixed and moving images')\n",
        "p1 = plt.subplot(121)\n",
        "p1.set_title('Fixed')\n",
        "plt.imshow(ref)\n",
        "\n",
        "p2 = plt.subplot(122)\n",
        "p2.set_title('Moving')\n",
        "plt.imshow(tst)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK6lqtmEL76X",
        "colab_type": "text"
      },
      "source": [
        "We now put it all together. Note that we reuse some of our code in the previous tutorial on adaptive stochastic gradient techniques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTe8ru8eRNS6",
        "colab_type": "code",
        "outputId": "e85151a1-4713-44c7-f424-c6df47674bb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Next cell\n",
        "x = f - m # This is the \"error\" between the images.\n",
        "C = np.array([[36, np.sum(x)], [np.sum(x), np.sum(x**2)]])/36\n",
        "\n",
        "# Run the previous cells before running this one.\n",
        "mu_0 = np.array([0.1, 0.1])\n",
        "L = 3             # After how many iterations will we recompute the gradient.\n",
        "K = 25            # Maximum number of times we will update the gradient.\n",
        "x = f - m         # This is the \"error\" between the images.\n",
        "n_samples = 6     # Number of samples to compute the stochastic gradient.\n",
        "\n",
        "# Parameters mentioned in equation (5) of the paper.\n",
        "theta_0 = 0.05\n",
        "H_00 = theta_0 * np.eye(len(mu_0))\n",
        "g_0 = np.ones(len(mu_0))\n",
        "\n",
        "# We will keep the implementation simple by choosing the step size to be 1.\n",
        "\n",
        "def compute_approx_g(x, mu):\n",
        "  g = np.zeros(2)\n",
        "  for j in range(len(x)):\n",
        "    # Recall that the data for the fixed image is in the array x\n",
        "    X = np.array([1, x[j]]) \n",
        "    coeff = -2 * ((1 - mu[1]) * x[j] - mu[0])\n",
        "    g = g + coeff * X\n",
        "    \n",
        "  return g\n",
        "\n",
        "import random\n",
        "\n",
        "mu_prev = mu_0\n",
        "H_prev = H_00\n",
        "g_prev = g_0\n",
        "m_prev = m\n",
        "\n",
        "for k in range(K):\n",
        "  g_sum = np.zeros(2) # The sum of gradients. We will use it for denoising.\n",
        "\n",
        "  # Select a new set of pixels for computation of each approximate gradient\n",
        "  S = random.sample(range(0, 36), n_samples)\n",
        "  for l in range(L):\n",
        "    mu_curr = mu_prev - np.dot(H_prev, g_prev)\n",
        "    m_curr = m_prev * mu_curr[0] + mu_curr[1]\n",
        "    x = m_curr - f\n",
        "    \n",
        "    g_curr = compute_approx_g(x, mu_curr)       \n",
        "    s = mu_curr - mu_prev\n",
        "    y = g_curr - g_prev\n",
        "\n",
        "    rho = 1/np.dot(y, s)\n",
        "    V = np.eye(len(mu_0)) - rho * np.outer(y, s)\n",
        "    Vt = V.transpose()\n",
        "\n",
        "    H_prev = np.dot(np.dot(Vt, H_prev), V) + rho * np.outer(s, s)\n",
        "    g_sum = g_sum + g_prev\n",
        "    g_prev = g_curr\n",
        "    mu_prev = mu_curr\n",
        "\n",
        "  avg_g = g_sum/L\n",
        "  mu_curr = mu_prev - np.dot(H_prev, avg_g)\n",
        "  m_curr = m_prev * mu_curr[0] + mu_curr[1]\n",
        "\n",
        "  x = m_curr - f\n",
        "  s = mu_curr - mu_prev\n",
        "  y = avg_g - g_prev\n",
        "\n",
        "  rho = 1/np.dot(y, s)\n",
        "  V = np.eye(len(mu_0)) - rho * np.outer(y, s)\n",
        "  Vt = V.transpose()\n",
        "\n",
        "  H_prev = np.dot(np.dot(Vt, H_prev), V) + rho * np.outer(s, s)\n",
        "  g_prev = avg_g\n",
        "  mu_prev = mu_curr\n",
        "\n",
        "print(f'Transformation parameters are mu = {mu_curr}')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transformation parameters are mu = [-0.67889137  1.3275286 ]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}