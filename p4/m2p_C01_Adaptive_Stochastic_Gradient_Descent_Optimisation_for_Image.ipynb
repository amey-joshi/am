{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "m2p_C01_Adaptive Stochastic Gradient Descent Optimisation for Image.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM13k27NCYAAILkr8cj2ik+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amey-joshi/am/blob/master/p4/m2p_C01_Adaptive_Stochastic_Gradient_Descent_Optimisation_for_Image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WwW6g03iivj",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "**Image registration** consists of matching two images of the same object that are taken in different configurations, or at different times or using different cameras. An example of the first case is when we are trying to match an image of an object in motion with the same object at rest. An example of the second case is a satellite taking images of the same patch of earth's surface at different times. An example of the third case is when one is trying to match a CT image of a portion of a certain patient with an MR image of the same portion of the same patient. In each case, we want to transform one of the images into another in 'the best possible' way. The word 'best' indicates that mathematically, image registration is an optimization problem.\n",
        "\n",
        "In this tutorial, we will use the first case as an example of image registration. That is, we will try to match a moving image with a fixed one. Define the fixed image by the mapping $F: \\Omega_F \\subset \\mathbb{R}^D \\mapsto \\mathbb{R}$. Thus, the function $F$ maps a point $x$ in the subset $\\Omega_F$ of $D$-dimensional points to a real number. Similarly, let the moving image be defined by the mapping $M: \\Omega_M \\subset \\mathbb{R}^D \\mapsto \\mathbb{R}$. Consider the mapping $T: \\Omega_F \\times \\mathbb{R}^p \\mapsto \\Omega_M$, where $\\mathbb{R}^p$ is the space of parameters. If $\\mu \\in \\mathbb{R}^p$ then $T(x, \\mu) = y$ maps a point $x \\in \\Omega_F$, the domain of the fixed image and a certain values of parameters $\\mu$ to a point $y \\in \\Omega_M$. \n",
        "\n",
        "Before proceeding, let us spend some time understanding this mathematical notation. As a concrete example, let $D = 3$, that is $\\Omega_F$, the domain of the fixed image, is a subset of the three dimensional euclidean space. The transformation $T$ will typically have a certain number of parameters. For example, we may try to fit $ax + b$ to $y$. Thus, we want to find the 'best' $a$ and $b$ so that $ax + b$ is closest to $y$. In this case, the transformation space if two dimensional, or $p = 2$.\n",
        "\n",
        "The idea of a match is expressed in terms of a cost function $\\mathcal{C}$. A commonly used cost function is the root mean square error between the fixed and the moving images. Thus, we may define\n",
        "$$\n",
        "  \\mathcal{C}(F, M) = \\left(\\sum_{x \\in \\Omega_F}||x - T(x, \\mu)||^2\\right)^{1/2},\n",
        "$$\n",
        "where $F$ is the fixed image and $M$ is the moving image. Note that in order to compute $\\mathcal{C}(X, Y)$, we have to add $||x - T(x, \\mu)||^2$ over **all** points and then take the square root. A typical $512 \\times 512$ image will involve summing over $512^2 = 261144$ points. Thus, computation of $C$ is usually an expensive operation.\n",
        "\n",
        "The optimization problem is to find those parameters $\\mu \\in \\mathbb{R}^p$ that **minimize** the cost $\\mathcal{C}(F, M)$. Note the similarity between this problem and the simple case of fitting a line using least square method. The image registration problem can be mathematically expressed as\n",
        "$$\\tag{1}\n",
        "\\hat{\\mu} = \\mathrm{argmin}_{\\mu}\\;\\mathcal{C}(F, M \\circ T).\n",
        "$$\n",
        "Equation (1) says, $\\hat{\\mu}$ is that value of the parameters that minimizes $\\mathcal{C}(F, M \\circ T)$. Let us understand the $M \\circ T$ part. If $x$ is a point in the domain $\\Omega_F$ of the fixed image then $(M \\circ T)(x) = M(T(x, \\mu))$. For a certain value of the parameters $\\mu$, $T$ maps a point $x \\in \\Omega_F$ to a point, say $y$ in the domain $\\Omega_M$ of the moving image. Recall the definition of the transformation $T$ as a mapping $T: \\Omega_F \\times \\mathbb{R}^p \\mapsto \\Omega_M$. Thus $M(T(x, \\mu)) = M(y)$, a point in the moving image. Thus, the composite function $M \\circ T$ maps a point of the fixed image to a point in the moving image. The optimization problem consists in finding the set of parameters which minimize the cost function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugaPGpiXsB5Y",
        "colab_type": "text"
      },
      "source": [
        "## Gradient descent\n",
        "\n",
        "This tutorial uses the **adaptive stochastic gradient descent algorithm** to solve the optimization problem defined by equation (1). We shall explain the algorithm by telling its evolution from **gradient descent** to **stochastic gradient descent** and finally to **adaptive stochastic gradient descent** by explaining the ideas using an example of a simple function defined over one variable. Consider the function shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nSUBDBAveVW",
        "colab_type": "code",
        "outputId": "d7b30697-8e42-46c2-bec4-71db837dba8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.linspace(0.1, 1.1, 100)\n",
        "y = np.divide(np.cos(3 * np.pi * x), x)\n",
        "\n",
        "def f(x):\n",
        "  return np.cos(3 * np.pi * x)/x\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.xlabel(r'$x$')\n",
        "plt.ylabel(r'$y$')\n",
        "plt.title(r'$y = \\cos(3\\pi x)/x$')\n",
        "plt.text(0.15, f(0.15), 'A')\n",
        "plt.text(0.5, f(0.5), 'B')\n",
        "plt.text(0.62, f(0.62), 'C')\n",
        "plt.text(0.7, f(0.7), 'D')\n",
        "plt.text(0.3, f(0.3), 'M1')\n",
        "plt.text(1, f(1), 'M2')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXQZPkNlyolI",
        "colab_type": "text"
      },
      "source": [
        "The idea of 'gradient descent' is just what is described by its name - descend against the gradient. The gradient is the direction in which the function rises the most. In a gradient descent, we take the opposite direction, in which it the function falls the most. Thus, we start at the point A in the graph shown above, we will reach M1. We will reach M1 even if we start from the opposite side, from point B. Point C is an interesting position. One can start in either direction and keep descending. If we descend to the left we reach M1 otherwise we reach M2. Both are correct answers for a gradient descent promises us to take to a **local minimum**.\n",
        "\n",
        "There is no general algorithm that assures that we hit a **global minimum**. If the function is **convex** we have the happy situation of the local minimum is also the global minimum. However, few functions in the context of images are convex. We will illustrate the gradient descent algorithm with a simple example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kFBqJiS04jr",
        "colab_type": "code",
        "outputId": "d3e068a1-3a32-42e7-b71f-80287da6a468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def f(x):\n",
        "  return np.cos(3 * np.pi * x)/x\n",
        "  \n",
        "def grad_f(x):\n",
        "  y = 3*np.pi*x\n",
        "  return -(y*np.sin(y) + np.cos(y))/x**2\n",
        "\n",
        "EPSILON = 1e-4\n",
        "max_iter = 1000\n",
        "n = 0\n",
        "x = 0.15 # Starting point.\n",
        "curr = f(x)\n",
        "step_size = 0.0005\n",
        "found = False\n",
        "\n",
        "while n < max_iter:\n",
        "  n = n + 1\n",
        "  df = grad_f(x)\n",
        "\n",
        "  if df < 0:\n",
        "    x = x + step_size\n",
        "  else:\n",
        "    x = x - step_size\n",
        "\n",
        "  new = f(x)\n",
        "  if np.abs(new - curr) < EPSILON:    \n",
        "    found = True\n",
        "    break\n",
        "  else:\n",
        "    curr = new\n",
        "\n",
        "if found:\n",
        "  print(f'Minimum is {x}. Needed {n} iterations')\n",
        "else:\n",
        "  print('Could not find a minimum of the function.')\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum is 0.2965000000000001. Needed 293 iterations\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2iY0VWi7Wq4",
        "colab_type": "text"
      },
      "source": [
        "The code in the previous cell is written only for sake of illustration. A general purpose optimization function has a far more complicated implementation. You can experiment by changing the values of  <code>EPSILON, step_size or max_iter.</code> . If the <code>step_size</code> is too large, one might miss the minimum M1 and land at M2. If <code>EPSIPLON</code> is too small one might not find the minimum in the number of iterations less than <code>max_iter</code>. If <code>max_iter</code> is too small than one might give up the search before reaching the minimum. These observations indicate that writing a general purpose optimization function is a challenging task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2k9rfeJ_XWb",
        "colab_type": "text"
      },
      "source": [
        "## Stochastic gradient descent\n",
        "\n",
        "In the simple example we considered, it was very easy to compute the gradient. But often times, computing the gradients of cost functions like the ones in equation (1) is quite expensive. Usually, computing the cost function or its gradient involves all data points. When the data size is large and the data are multi-dimensional, computation of the gradient of the cost function at each iteration becomes a significant bottleneck. The **stochastic gradient descent** algorithm approximates the computation of gradient of a function by using only a small sample of randomly chosen data points. For example, in the case of linear regression, the cost function is \n",
        "$$\\tag{2}\n",
        "C_{r} = \\sum_{i=1}^N||\\hat{y}_i - y_i||^2,\n",
        "$$\n",
        "where $y_i \\in \\mathbb{R}^n$ are the observed valuesand $\\hat{y}_i$ are the estimated values. The norm $||\\cdot||$ is the usual norm in $\\mathbb{R}^n$. Computation of this term needs $Nn$ multiplications and $N(n-1)$ additions. If $N$ is large then computation of $C_r$ is an expensive task. The stochastic gradient descent approximates equation (2) as\n",
        "$$\\tag{3}\n",
        "C_{r} \\approx \\sum_{i=1}^M||\\hat{y}_i - y_i||^2,\n",
        "$$\n",
        "where $M \\ll N$ and the points $y_i$ are chosen randomly at each iteration. Although approximation of equatino (3) appears quite drastic in practice it is quite effective. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5F8aZ13IN-V",
        "colab_type": "text"
      },
      "source": [
        "## Adaptive stochastic gradient descent (ASGD)\n",
        "\n",
        "The step size of in the stochastic gradient determines how much we will descend in each iteration of the algorithm. Sometimes, it is useful to have the step size vary. If the change in the value of the function is not too much we increase the step size. If it is too small, we reduce it. Recall the experiment suggested on the gradient descent algorithm. A large step size can easily miss the minimum M1 and can land at M2 instead. Thus, the algorithm is quite sensitive to the choice of step size. Adaptive stochastic gradient algorithm improves the stochastic gradient algorithm by selecting an appropriate step size at each iteration.\n",
        "\n",
        "The adaptive stochastic gradient algorithm described in this paper can be summarized in the equations\n",
        "$$\\tag{4}\n",
        "\\boldsymbol{\\mu}_{k+1} = \\boldsymbol{\\mu}_k - \\gamma_k \\tilde{\\boldsymbol{g}}_k,  k = 0, 1, \\ldots, K,\n",
        "$$\n",
        "where\n",
        "$$\\tag{5}\n",
        "\\tilde{\\boldsymbol{g}}_k = \\boldsymbol{g}_k + \\boldsymbol{\\epsilon}_k.\n",
        "$$\n",
        "The 'stochastic' nature comes from the fact that the true gradient $\\boldsymbol{g}_k$ is approximated by $\\tilde{\\boldsymbol{g}}_k$. The 'adaptive' nature follows from the fact that the step size $\\gamma_k$ changes for every iteration.\n",
        "\n",
        "The step size is chosen by computing the inner product of the gradient in the current step with gradient in the previous step. If the two gradients are in the same direction then we can afford to take a longer step. If they are not then the step size should be taken small. The rest of the tutorial explains the details underlying this idea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI2w3AVwEuOn",
        "colab_type": "text"
      },
      "source": [
        "# Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN6xPAF_E4uE",
        "colab_type": "text"
      },
      "source": [
        "## Summary of ASGD\n",
        "\n",
        "The adaptive step size is determined as\n",
        "$$\\tag{6}\n",
        "\\gamma_k = \\gamma(t_k),\n",
        "$$\n",
        "where the **gain function** $\\gamma(\\cdot)$ is defined as\n",
        "$$\\tag{7}\n",
        "\\gamma(x) = \\frac{a}{(x + A)^\\alpha},\n",
        "$$\n",
        "$a, A, \\alpha$ are constants and \n",
        "$$\\tag{8}\n",
        "t_{k+1} = [t_k + f(-\\boldsymbol{g}_k^T\\boldsymbol{g}_{k-1})]^+.\n",
        "$$\n",
        "This paper chooses $$\\tag{9} \\alpha = 1$$\n",
        "\n",
        "Note that $\\boldsymbol{g}_k^T\\boldsymbol{g}_{k-1}$ is just the inner product of $\\boldsymbol{g}_k$ and $\\boldsymbol{g}_{k-1}$. $[x]^+ = \\max(0, x)$ and $f$ has the  characteristics of the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function),\n",
        "$$\\tag{10}\n",
        "f(x) = \\frac{1}{1 + e^{-x}}.\n",
        "$$\n",
        "We emphasize that $f$ is **not** the sigmoid function. It is defined as\n",
        "$$\\tag{11}\n",
        "f(x) = f_{MIN} + \\frac{f_{MAX} - f_{MIN}}{1 - (f_{MAX}/f_{MIN})e^{-x/\\omega}},\n",
        "$$\n",
        "where $f_{MAX} > 0$, $f_{MIN} < 0$ and $\\omega > 0$ are constants. We demonstrate the function for $f_{MAX} = 1, f_{MIN} = -0.5$  and several choices of $\\omega$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX1g42vjK5Q6",
        "colab_type": "code",
        "outputId": "91b9f8b5-3472-4e4e-ef4a-c18d6f52e21f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f_min = -0.5\n",
        "f_max = 1.0\n",
        "\n",
        "def sigmoid_like(x, omega):\n",
        "  y = np.exp(-x/omega)\n",
        "  return f_min + (f_max - f_min)/(1 - (f_max/f_min) * y)\n",
        "\n",
        "x = np.linspace(-15, 15, 101)\n",
        "y = sigmoid_like(x, 0.1)\n",
        "\n",
        "plt.plot(x, sigmoid_like(x, 0.1), label = r'$\\omega = 0.1$')\n",
        "plt.plot(x, sigmoid_like(x, 0.5), label = r'$\\omega = 0.5$')\n",
        "plt.plot(x, sigmoid_like(x, 1), label = r'$\\omega = 1$')\n",
        "plt.plot(x, sigmoid_like(x, 5), label = r'$\\omega = 5$')\n",
        "plt.plot([-15, 15], [0, 0], '--', color = 'black')\n",
        "plt.plot([0, 0], [-0.6, 1.1], '--', color = 'black')\n",
        "plt.xlabel(r'$x$')\n",
        "plt.ylabel(r'$y$')\n",
        "plt.legend()\n",
        "plt.title('Examples of sigmoid-like functions')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Examples of sigmoid-like functions')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eL3ACu_cc7h",
        "colab_type": "text"
      },
      "source": [
        "## Application of ASGD\n",
        "\n",
        "Although equation (9) allowed us to fix $\\alpha$, we still have five more parameters to choose before we implement the algorithm. They are 1) $a$, 2) $A$, 3) $f_{MIN}$, 4) $f_{MAX}$ and 5) $\\omega$. The following assumptions will aid in selecting their values.\n",
        "\n",
        "\n",
        "\n",
        "*   **Assumption B1** The errors $\\boldsymbol{\\epsilon}_k$ in equation (5) are assumed to be independent and identically distributed with mean zero and a finite variance. This tutorial also assumes that they are normally distributed.\n",
        "*   **Assumption B2** The gain function $\\gamma$ is positive and monotone descreasing over $[0, \\infty)$. Thus, $\\gamma(0)$ is its minimum. It has two additional properties\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\int_0^\\infty \\gamma(t)dt &=& \\infty \\tag{12} \\\\\n",
        "\\int_0^\\infty \\gamma^2(t) dt &<& \\infty \\tag{13}\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "*   **Assumption B3** The cost function $\\mathcal{C}$ has a unique minimum $\\hat{\\boldsymbol{\\mu}}$. That is $\\mathcal{C}$ is a **convex function**. $\\mathcal{C}$ is also assumed to be twice differentiable throughout its domain (differentiability guarantees continuity) and for all $\\boldsymbol{\\mu}$, the eigenvalues of the hessian\n",
        "$$\\tag{14}\n",
        "\\boldsymbol{H} = \\frac{\\partial^2\\mathcal{C}}{\\partial\\mu_i\\partial\\mu_j}\n",
        "$$\n",
        "are bounded by a positive constant $\\lambda$. The optimization problem then can be solved for all starting points $\\boldsymbol{\\mu}_0$ as long as $\\gamma_k < \\gamma(0)$ for all $k$. It is further assumed that there exists $R > 0$, $\\beta_0 > 0$ such that \n",
        "$$\\tag{15}\n",
        "||\\boldsymbol{g}(\\boldsymbol{\\mu})||^2 > \\frac{\\gamma(0)}{2}\\lambda(||\\boldsymbol{g}(\\boldsymbol{\\mu})||^2 + \\mathrm{tr}(\\boldsymbol{\\Sigma})) + \\beta_0,\n",
        "$$\n",
        "where $\\boldsymbol{\\Sigma}$ is the variance matrix of the errors $\\boldsymbol{\\epsilon}_k$ and $\\mathrm{tr}(\\cdot)$ stands for the trace operator. Equation (15) is true for all $\\boldsymbol{\\mu}$ that satisfy the relation $||\\boldsymbol{\\mu} - \\hat{\\boldsymbol{\\mu}}|| \\ge R$.\n",
        " \n",
        "*   **Assumption B4** The function $f$ is monotone increasing, continuous and bounded with $f_{MAX} > 0$ as the maximum and $f_{MIN}$ as the minimum. It is also assumed that \n",
        "$$\\tag{16}\n",
        "E_0 = \\mathbb{E}f(\\boldsymbol{\\epsilon}_k^T\\boldsymbol{\\epsilon}_{k-1}) > 0.\n",
        "$$\n",
        "*   **Assumption B5** These assumptions are necessary to prove asymptotic normality,\n",
        "$$\\tag{17}\n",
        "\\gamma(t) = \\frac{a}{t + A}.\n",
        "$$\n",
        "and the eigenvalues of the matrix \n",
        "$$\\tag{18}\n",
        "\\boldsymbol{W} = \\frac{\\boldsymbol{I}}{2} - \\frac{a}{E_0}\\boldsymbol{H}(\\hat{\\boldsymbol{\\mu}})\n",
        "$$\n",
        "are positive. Here $E_0$ is defined by equation (16)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyZ84iAto9fv",
        "colab_type": "text"
      },
      "source": [
        "## Distribution estimates\n",
        "\n",
        "We need some more assumptions about the distributions of the gradients in order to choose the correct values of the five parameters mentioned in the previous section. We mention them in this section.\n",
        "\n",
        "A general cost function is of the form\n",
        "$$\\tag{19}\n",
        "\\mathcal{C} = \\Psi\\left(\\frac{1}{|\\Omega^\\prime_F|}\\sum_{x_i \\in \\Omega^\\prime_F}\\xi(F(x_i), M(T(x_i, \\mu)))\\right),\n",
        "$$\n",
        "where $\\Psi: \\Xi \\mapsto \\mathbb{R}$ and $\\xi: \\mathbb{R} \\times \\mathbb{R} \\mapsto \\Xi$ are differentiable functions, $\\Omega^\\prime_F$ is the is a subset of pixels/voxels of the fixed image and $|\\Omega^\\prime_F|$ is its cardinality. Usually, $\\Xi = \\mathbb{R}$ but it can be a multi-dimensional set like $\\mathbb{R}^P$.\n",
        "\n",
        "An example of these functions and domains is $\\Xi = \\mathbb{R}$, $\\xi(u, v) = (u - v)^2)$ and $\\Psi(w) = w$. \n",
        "\n",
        "The ASGD algorithm requires the gradient of the cost function. It is\n",
        "$$\\tag{20}\n",
        "\\boldsymbol{g} = \\frac{\\partial\\mathcal{C}}{\\partial\\boldsymbol{\\mu}} = \n",
        "\\frac{1}{|\\Omega^\\prime_F|}\\sum_{x_i \\in \\Omega^\\prime_F}\\left(\\frac{\\partial T}{\\partial\\boldsymbol{\\mu}}\\right)^T\\frac{\\partial M}{\\partial\\boldsymbol{x}}\\frac{\\partial\\xi}{\\partial v}\\frac{\\partial\\Psi}{\\partial u}.\n",
        "$$\n",
        "\n",
        "In order to estimate the distribution of the gradients, we need two more assumptions.\n",
        "\n",
        "\n",
        "*   **Assumption A1** $\\left({\\partial T}/{\\partial\\boldsymbol{\\mu}}\\right)$  is independent of $\\boldsymbol{\\mu}$. That is, $T$ is a linear function of the parameters $\\boldsymbol{\\mu}$. Note the similarity of this assumption with that of linear models in statistics. We define\n",
        "$$\\tag{21}\n",
        "\\boldsymbol{J}_i = \\frac{\\partial T}{\\partial\\boldsymbol{\\mu}}(x_i, \\boldsymbol{\\mu}).\n",
        "$$\n",
        "*   **Assumption A2** If\n",
        "$$\\tag{22}\n",
        "z_i = \\frac{\\partial M}{\\partial\\boldsymbol{x}}\\frac{\\partial\\xi}{\\partial v}\\frac{\\partial\\Psi}{\\partial u}\n",
        "$$\n",
        "then $z_i$ are normally distributed with parameters $0$ and $\\sigma^2\\boldsymbol{I}$ for some constant $\\sigma$. \n",
        "\n",
        "With these assumptions the it can be shown that\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\boldsymbol{g} &\\sim& \\mathcal{N}(0, \\sigma_1^2 C) \\tag{23} \\\\\n",
        "\\tilde{\\boldsymbol{g}}_k &\\sim& \\mathcal{N}(0, \\sigma_1^2 C) \\tag{24} \\\\\n",
        "{\\boldsymbol{\\epsilon}}_k &\\sim& \\mathcal{N}(0, \\sigma_3^2 C), \\tag{25}\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "where $\\sigma_1^2, \\sigma_2^2$ and $\\sigma_3^2$ are constants and\n",
        "$$\\tag{26}\n",
        "C = \\frac{1}{|\\Omega^\\prime_F|}\\sum_{x_i \\in \\Omega^\\prime_F}\\boldsymbol{J}_i^T\\boldsymbol{J}_i\n",
        "$$\n",
        "The quantity $\\boldsymbol{J}_i$ is defined in equation (21).\n",
        "We further define,\n",
        "$$\\tag{27}\n",
        "\\boldsymbol{\\Sigma} = \\sigma_3^2 C.\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBOAmZqmClVx",
        "colab_type": "text"
      },
      "source": [
        "# Implementation of AGSD\n",
        "\n",
        "We will implement the algorithm on two binary images that we generate in the code snippet below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1jk7R4NDF12",
        "colab_type": "code",
        "outputId": "87895eeb-b4d1-4e07-ef0d-3c1a05aa24ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "f = np.zeros(36)\n",
        "f[14] = f[15] = f[20] = f[21] =  f[35] = 128\n",
        "ref = Image.new('1', (6, 6))\n",
        "ref.putdata(f)\n",
        "\n",
        "m = np.zeros(36)\n",
        "m[7] = m[8] = m[13] = m[14] = m[28] = 128\n",
        "tst = Image.new('1', (6, 6))\n",
        "tst.putdata(m)\n",
        "\n",
        "plt.figure()\n",
        "plt.suptitle('Fixed and moving images')\n",
        "p1 = plt.subplot(121)\n",
        "p1.set_title('Fixed')\n",
        "plt.imshow(ref)\n",
        "\n",
        "p2 = plt.subplot(122)\n",
        "p2.set_title('Moving')\n",
        "plt.imshow(tst)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD1CAYAAAB5n7/BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAUEklEQVR4nO3dfbBkdX3n8feHAUQEgURlgYFBA7Gk0GiF4FaiGxLJgoqRJFVEjKBGa4zZ8FAJRTAVjVpSqaSyqzGJZbkWEXUnOFkeKlIhGKvwMREBn8JAUGBgB5g4GkRmhIjgd/84Z9Zmlntv951++HXP+1XVNd3ndJ/zPXe+93N//euHk6pCktSuvWZdgCRpeQa1JDXOoJakxhnUktQ4g1qSGmdQS1LjDOoFkGRHkmeNeZsnJblnnNtcZl8fSvKuaexrmRpenOS2CW37miSvncS2tWfYe9YFaHhJ7gIOBR4bWPyTVXXAbCpaHFX1WeDZE9r2SyexXe05DOr584qq+uSsi5A0PU59LIAkleSYJPsm+UqSc/rla5J8Psnb+tuHJ7k8ybeSbE5y7sA2ntxPQXwnyS3Az6ywzz9PsiXJg0luSvLigXVvT7IxyYeTbE+yKckJA+tfkORL/bqPAfsts5/X9cfw7iQPJLkzyc/2y7ck2TY4rZDkoH6/30pyd5I/TLJXkif1jz9+4L5PT/JwkmfsOtWT5K4kFyT5WpLvJvlYkv0G1l+YZGuS+5K8cef/wRLH8Kkkb1zl8bw8yZf7n/OWJG/fZdtn98f570ne2td9cr9uryQXJbmjX78xyY/16/ZL8tF++QNJbkhy6HL/55odg3qBVNUjwGuAdyZ5DnARsAa4OMlewMeBrwJHAC8Bzk9ySv/wPwJ+or+cAqw0p3oD8Hzgx4ANwN8OBhnwy8BlwMHA3wF/CZBkX+Aq4CP9Y/8W+LUV9vVC4GvAj/f7uozuD8kx/fH+ZZKd0z9/ARwEPAv4eeBs4PVV9X3gCuDMge2eAXy6qrYtsd8zgFOBZwLPA17XH8OpwO8CJ/c1nLRC/btzPN/rj+Fg4OXAm5Oc3tdxHPA+4DeAw/rjPmJgP+cAp/c/h8OB7wB/1a97bX//I/s6fgt4eMTj0LRUlZc5uQB3ATuAB/rLVf3yAo4ZuN/vAbfR/WIe2y97IfB/dtneW4C/7q/fCZw6sG49cM8ItX0H+Kn++tuBTw6sOw54uL/+X4D7gAys/yfgXUts93XANwZuP7c/3kMHlv073R+NNcAjwHED694EfKq/fjJwx8C6zwNn99dPGjze/mf9moHbfwq8v79+CfDHA+uO2fX/YJdj+BTwxlGPZ4ltvQd4d3/9bcDfDKzbvz/+k/vbtwIvGVh/GPADuinP3+x/7s+bdV97WfniiHr+nF5VB/eX05e4z6XAOuDvq+ob/bJ1wOH909wHkjwA/AHdi5PQjbi2DGzj7uWK6KcFbu2nBR6gG509beAu/zZw/SFgvyR79/u5t/rkGGZfwDcHrj8MUFW7Ljug3/8+u2zvbn40yrwO2D/JC5McTRfuVy6z312PYecod9ef1eD1YQx7PPS1XtdP5XyXbuS78+f8uDqq6iG6kN9pHXDlwP/3rXQvRB9K94zmWuCyfvrmT5PsM+JxaEoM6sX0PuBq4JQkL+qXbQE2D4T8wVV1YFW9rF+/le5p8E5HLbXxfj76QrqpgUOq6mDgu0CGqG0rcESSwfsuua8RfZtuxLhul23fC1BVjwEb6aY/zgSurqrtq9jPVmDtwO0jl7rjGGygmzo6sqoOAt7Pj37Oj6sjyZPppjF22gK8dJf/8/2q6t6q+kFVvaOqjgN+FjiNbopFDTKoF0ySs4CfpnuKfS5waT/f+UVge5LfT/fC4ZokxyfZ+aLhRuAtSQ5JspZufnMpBwKPAt8C9k73YuVThyzxn/vHnptknyS/Cpw44mE+oYEgvjjJgUnW0c0lf3TgbhuAX6eb192wyl1tBF6f5DlJ9gfeuhtlr+RA4P6q+o8kJwKvHlj3v4FX9C9G7ks35TT4B/D9dD+LdfD/Xjx9ZX/9F5I8N8ka4EG6P3A/nOBxaDcY1AskyVF0c5hnV9WOqtoA3Eg3p/kY3ajp+cBmutHnB+mmLADeQTdNsBn4BN1T46VcC/wD8PX+Mf/BkE//q3vB81fp/pDcTxeaVwx9kCs7h+4FuDuBz9GF8SUD+7++X384cM1qdlBV1wDvpZtKuR34Qr/q+6uuemm/Tffi8Ha6OemNA3Vsojvey+hG1zuAbQN1/DndaPwT/eO/QPdaBcB/ogv6B+mmRD7N8v/nmqE8fqpQ0qj6d9jcDDypqh6dYR0H0L3IfGxVbZ5VHRo/R9TSKiT5lf692YcAfwJ8fBYhneQVSfZP8hTgz4B/oXvHihaIQS2tzpvophnuoHsnxZtnVMcr6d7ueB9wLPCq8mnywnHqQ5Ia54hakhpnUEtS4wxqSWqcQS1JjTOoJalxBrUkNc6glqTGGdSS1DiDWpIaZ1BLUuMMaklqnEEtSY0zqCWpcQa1JDXOoJakxhnUktQ4g1qSGmdQS1LjDGpJapxBLUmNM6glqXEGtSQ1zqCWpMYZ1JLUOINakhpnUEtS4wxqSWqcQS1JjTOoJalxBrUkNc6glqTGGdSS1DiDeoKS7EjyrDFv86Qk94xzm9KkJXlxkttmXce8MqjHJMldSR7uw3lHkh3AT1bVnbOuTRpG38OPJHnaLsu/nKSSHL3abVfVZ6vq2btb457KoB6vV1TVAQOX+2ZdkDSizcCZO28keS6w/+zKERjUE9WPQo5Jsm+SryQ5p1++Jsnnk7ytv314ksuTfCvJ5iTnDmzjyUk+lOQ7SW4BfmZGh6M9w0eAswduvxb48M4bSQ5K8uG+V+9O8odJ9krypCQPJDl+4L5P759lPmPXKbt+9H5Bkq8l+W6SjyXZb2D9hUm2JrkvyRt3/i5N+NibZVBPQVU9ArwGeGeS5wAXAWuAi5PsBXwc+CpwBPAS4Pwkp/QP/yPgJ/rLKXS/ONKkfAF4apLnJFkDvAr46MD6vwAOAp4F/DxdqL++qr4PXMHAaBw4A/h0VW1bYl9nAKcCzwSeB7wOIMmpwO8CJwPHACeN48DmmUE9Xlf1o4oHklw1uKKqbgbeBVwFXACcVVWP0Y2Qn15V76yqR/o57f9J9wsCXTNfXFX3V9UW4L1TOxrtqXaOqn8JuBW4t1++M7jfUlXbq+ou4L8DZ/XrN/CjvgV4db9sKe+tqvuq6n66wcrz++VnAH9dVZuq6iHg7bt9RHNu71kXsGBOr6pP7ryRpHZZfylwMXB5VX2jX7YOODzJAwP3WwN8tr9+OLBlYN3d4y1Z+v98BPgM3Uj3wwPLnwbsw+N78G66Z4IA1wH7J3kh8E264L1ymf3828D1h+h6nf7fGwfWDfb/Hsmgnq73AVcDpyR5UVV9jq4JN1fVsUs8ZitwJLCpv33U5MvUnqyq7k6yGXgZ8IaBVd8GfkA3uLilX3YU/Yi7qh5LspFu+uObwNVVtX0VJWwF1g7cPnIV21goTn1MSZKzgJ+mm4c7F7g0yQHAF4HtSX6/f+FwTZLjk+x80XAj8JYkhyRZC5wzi/q1x3kD8ItV9b2BZY/R9ePFSQ5Mso5uLnlwDnsD8OvAb7D8tMdyNgKv7+fJ9wfeusrtLAyDegqSHAW8Bzi7qnZU1Qa6p3bv7uepT6N7mriZbtTyQboXbADeQff0cjPwCbqnpdJEVdUdVXXjE6w6B/gecCfwObowvmTgcdf36w8Hrlnlvq+hey3mOuB2uhc4Ab6/mu0tglTtOo0qSe3o3yl1M/Ckqnp01vXMgiNqSc1J8iv9e7MPAf4E+PieGtJgUEtq05uAbcAddHPjb55tObPl1IckNc4RtSQ1zqCWpMZN5AMvT/CJPGmsqirT3qd9rUlbqq8dUUtS4wxqSWqcQS1JjTOoJalxBrUkNc6glqTGGdSS1LihgjrJqUluS3J7kosmXZQ0Lfa25sGK3/XRn+Dy63TnT7sHuAE4s6puWeYxfjBAEzWOD7yM2tv2tSZtdz7wciJwe1Xd2Z9N+zLgleMsTpoRe1tzYZigPoLHn1zyHn50Mktpntnbmgtj+66PJOuB9ePantQC+1otGCao7+XxZwFe2y97nKr6APABcC5Pc2PF3rav1YJhpj5uAI5N8swk+wKvAv5usmVJU2Fvay6sOKKuqkeT/A5wLbAGuKSqNk28MmnC7G3Ni4mcisuniJo0v49ai8jvo5akOWVQS1LjDGpJapxBLUmNM6glqXEGtSQ1zqCWpMaN7bs+JO2+SXyuYVaSqb/VfWE5opakxhnUktQ4g1qSGmdQS1LjDGpJapxBLUmNM6glqXEGtSQ1zqCWpMatGNRJLkmyLcnN0yhImhZ7W/NimBH1h4BTJ1yHNAsfwt7WHFgxqKvqM8D9U6hFmip7W/PCOWpJatzYvj0vyXpg/bi2J7XAvlYLMszXKiY5Gri6qo4faqPJ4nxXo5pUVWP5Ds1Rensafe3XnO7Zluprpz4kqXHDvD3vb4B/Bp6d5J4kb5h8WdLk2duaF0NNfYy8Uac+NGHjmvoYhVMfo3HqY3ROfUjSnDKoJalxBrUkNc6glqTGGdSS1DiDWpIaZ1BLUuPG9l0fi8r3tUqaNUfUktQ4g1qSGmdQS1LjDGpJapxBLUmNM6glqXEGtSQ1zqCWpMYNc4aXI5Ncl+SWJJuSnDeNwqRJs7c1L1Y8w0uSw4DDqupLSQ4EbgJOr6pblnnMwnycz08mtmkcZ3gZtbc9w8toFqnfpmXVZ3ipqq1V9aX++nbgVuCI8ZYnTZ+9rXkx0hx1kqOBFwDXT6IYaVbsbbVs6C9lSnIAcDlwflU9+ATr1wPrx1ibNBXL9bZ9rRYMdRbyJPsAVwPXVtX/GOL+CzPR5pxhm8Z1FvJRets56tEsUr9Ny1J9PcyLiQEuBe6vqvOH2ZlB3aZF+sUZ04uJI/W2QT2aReq3admdoH4R8FngX4Af9ov/oKr+fpnHLEy3+YvTpjEF9Ui9bVCPZpH6bVpWHdSrYVC3aZF+ccY19TEKg3o0i9Rv07Lqt+dJkmbLoJakxhnUktQ4g1qSGmdQS1LjDGpJapxBLUmNG/q7PiRNnu891hNxRC1JjTOoJalxBrUkNc6glqTGGdSS1DiDWpIaZ1BLUuMMaklq3IpBnWS/JF9M8tUkm5K8YxqFSZNmb2teDHvOxKdU1Y7+RKCfA86rqi8s85iFOU2FZ9xo0xjPmTh0by9SX6tNS/X1ih8hry6pdvQ39+kvNqzmnr2teTHUHHWSNUm+AmwD/rGqrp9sWdJ02NuaB0MFdVU9VlXPB9YCJyY5ftf7JFmf5MYkN467SGlSVupt+1otGPks5EneBjxUVX+2zH0W5umjc9RtmsRZyFfq7UXqa7Vp1WchT/L0JAf3158M/BLwr+MtT5o+e1vzYpjvoz4MuDTJGrpg31hVV0+2LGkq7G3NhZGnPoba6AI9RXTqo02TmPpYySL1tdq06qkPSdJsGdSS1DiDWpIaZ1BLUuMMaklqnEEtSY0zqCWpcQa1JDVumE8m7tEW6UMi0iKaxofSZp0DjqglqXEGtSQ1zqCWpMYZ1JLUOINakhpnUEtS4wxqSWqcQS1JjRs6qJOsSfLlJJ6qSAvDvtY8GGVEfR5w66QKkWbEvlbzhgrqJGuBlwMfnGw50vTY15oXw46o3wNcCPxwgrVI02Zfay6sGNRJTgO2VdVNK9xvfZIbk9w4tuqkCbGvNU+y0jdPJflj4CzgUWA/4KnAFVX1mmUeM/mvs9Ierap26+vM7OvFsUjfnrdUX68Y1I+7c3IScEFVnbbC/WxoTdTuBvUg+3q+7QlB7fuoJalxI42oh96oIw9N2DhH1MOyr9vkiFqSNHMGtSQ1zqCWpMYZ1JLUOINakhpnUEtS4wxqSWrc3rMuQItlGu9pPeGEEya+D82Pab3HeZYcUUtS4wxqSWqcQS1JjTOoJalxBrUkNc6glqTGGdSS1DiDWpIaN9QHXpLcBWwHHgMerSo/caCFYG9rHozyycRfqKpvT6wSaXbsbTXNqQ9JatywQV3AJ5LclGT9JAuSpszeVvOGnfp4UVXdm+QZwD8m+deq+szgHfomt9E1b5btbftaLRhqRF1V9/b/bgOuBE58gvt8oKpO8MUYzZOVetu+VgtWDOokT0ly4M7rwH8Fbp50YdKk2duaF8NMfRwKXNl/5+vewIaq+oeJViVNh72tubBiUFfVncBPTaEWaarsbc0L354nSY0zqCWpcQa1JDXOoJakxhnUktQ4g1qSGmdQS1LjDGpJatwo30c9im8Dd49w/6f1j1kEe/Sx9J/ym7R109jJExi1r2EP74dGtXocS/Z1qmqahTxxEcmNi/KlNx6LBi3Sz3BRjmUej8OpD0lqnEEtSY1rJag/MOsCxshj0aBF+hkuyrHM3XE0MUctSVpaKyNqSdISZh7USU5NcluS25NcNOt6VivJkUmuS3JLkk1Jzpt1TbsjyZokX05y9axrmUf2dbvmsbdnGtRJ1gB/BbwUOA44M8lxs6xpNzwK/F5VHQf8Z+C/zfGxAJwH3DrrIuaRfd28uevtWY+oTwRur6o7q+oR4DLglTOuaVWqamtVfam/vp2uEY6YbVWrk2Qt8HLgg7OuZU7Z142a196edVAfAWwZuH0Pc9wEOyU5GngBcP1sK1m19wAXAj+cdSFzyr5u11z29qyDeuEkOQC4HDi/qh6cdT2jSnIasK2qbpp1LWrHvPc1zHdvzzqo7wWOHLi9tl82l5LsQ9fM/6uqrph1Pav0c8AvJ7mL7in7Lyb56GxLmjv2dZvmtrdn+j7qJHsDXwdeQtfINwCvrqpNMytqldJ9G9GlwP1Vdf6s6xmHJCcBF1TVabOuZZ7Y1+2bt96e6Yi6qh4Ffge4lu5Fio3z2My9nwPOovsr/ZX+8rJZF6Xps681bn4yUZIaN+s5aknSCgxqSWqcQS1JjTOoJalxBrUkNc6glqTGGdSS1DiDWpIa938BDbHnNy2kXbYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGODrlXaDZDy",
        "colab_type": "text"
      },
      "source": [
        "## Estimation of parameters\n",
        "\n",
        "The five parameters mentioned in section <a href='Application of ASGD'>Application of ASGD</A> are estimated for a run of the algorithm. We do so in the steps below. Before we do so, we choose the user defined quantities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu_wO1J5D6Ui",
        "colab_type": "text"
      },
      "source": [
        "### User-defined quantities\n",
        "\n",
        "1.   We work with linear transformations of the form $$\\tag{28}\n",
        "T(x) = \\mu_0 + \\mu_1 x\n",
        "$$ where $\\mu_0, \\mu_1$ are the parameters of $T$. Thus, the goal of the optimization is to find the best values of these parameters so that when we apply $T$ to the moving image we get as close to the fixed image as possible. With this choice, our parameter space becomes $\\mathbb{R}^2$ and our parameter vector becomes\n",
        "$$\\tag{29}\n",
        "\\boldsymbol{\\mu} = (\\mu_0, \\mu_1).\n",
        "$$\n",
        "2.   Choose\n",
        "$$\\tag{30}\n",
        "\\delta = 0.1.\n",
        "$$\n",
        "This quantity was not introduced before. It appears in the theory of estimation of probability distribution of the gradients. We will find it useful while estimating the parameters of the ASGD algorithm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaDHwtiaFSxy",
        "colab_type": "text"
      },
      "source": [
        "### Estimation of parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK80IbK6FabR",
        "colab_type": "text"
      },
      "source": [
        "#### Computation of $C$.\n",
        "We use equation (26) to determine $C$. The set of all pixels of the fixed images has cardinality $36$. (Recall that we have generated $6 \\times 6$ binary images.) Therefore, $|\\Omega_F^\\prime| = 36$. From equation (28), the gradient of $T$ in the $\\mu$-space is \n",
        "$$\\tag{31}\n",
        "\\boldsymbol{J}_i = \\begin{pmatrix}1 & x_i\\end{pmatrix}\n",
        "$$\n",
        "so that\n",
        "$$\\tag{32}\n",
        "\\boldsymbol{J}_i^T \\boldsymbol{J}_i = \\begin{pmatrix}1 & x_i \\\\ x_i & x_i^2\\end{pmatrix}\n",
        "$$\n",
        "and\n",
        "$$\\tag{33}\n",
        "\\boldsymbol{C}(x_i) = \\sum_{x_i \\in \\Omega_F^\\prime} \\begin{pmatrix}1 & x_i \\\\ x_i & x_i^2\\end{pmatrix}.\n",
        "$$\n",
        "The code snippet below computes $C$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHM1fIEpGpFJ",
        "colab_type": "code",
        "outputId": "f62cfbfc-040a-4dbb-fb52-7344f1f4d888",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# This code snippet cannot be run independently. Please run all the cells above \n",
        "# before getting here.\n",
        "\n",
        "x = f - m # This is the \"error\" between the images.\n",
        "C = np.array([[36, np.sum(x)], [np.sum(x), np.sum(x**2)]])/36\n",
        "print(f'C = {C}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C = [[1.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 3.64088889e+03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5IdLP3dJZOv",
        "colab_type": "text"
      },
      "source": [
        "#### Computation of $\\sigma_4$\n",
        "$\\sigma_4$ too has not been introduced before and like $\\delta$ it appears in the theory of distributions of the gradients. It is defined as\n",
        "$$\\tag{34}\n",
        "\\sigma_4^2 = \\min_{x_j \\in \\Omega_F^\\prime}\\frac{\\delta^2}{||\\boldsymbol{J}_j||_F^2 + 2\\sqrt{2}||\\boldsymbol{J}_j\\boldsymbol{J}_j^T||_F},\n",
        "$$\n",
        "where $||\\cdot||_F$ stands for [Frobenius norm](https://mathworld.wolfram.com/FrobeniusNorm.html). Recall that we have selected $\\delta = 0.1$ in equation (30). From equation (31) it is clear that\n",
        "$$\\tag{35}\n",
        "\\boldsymbol{J}_j\\boldsymbol{J}_j^T = 1 + x_j^2\n",
        "$$\n",
        "so that\n",
        "$$\\tag{36}\n",
        "||\\boldsymbol{J}_j\\boldsymbol{J}_j^T||_F = \\sqrt{(1 + x_j^2)^2} = 1 + x_j^2.\n",
        "$$\n",
        "Similarly\n",
        "$$\\tag{37}\n",
        "||\\boldsymbol{J}_i||_F = \\sqrt{1 + x_j^2}.\n",
        "$$\n",
        "Equation (34) now becomes\n",
        "$$\\tag{38}\n",
        "\\sigma_4^2 = \\min_{x_j \\in \\Omega_F^\\prime}\\frac{\\delta^2}{1 + x_j^2 + 2\\sqrt{2}(1 + x_j^2)} = \\frac{1}{(1 + 2\\sqrt{2})}\\min_{x_j \\in \\Omega_F^\\prime}\\frac{\\delta^2}{(1 + x_j^2)}\n",
        "$$\n",
        "We compute $\\sigma_4$ in the code snippet below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEsGw4L_OvFI",
        "colab_type": "code",
        "outputId": "2f471679-d11f-4831-ab17-fba6be39511c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# This code snippet cannot be run independently. Please run all the cells above \n",
        "# before getting here.\n",
        "\n",
        "DELTA = 0.1\n",
        "den_sigma_4 = np.zeros(len(x))\n",
        "delta_sq = DELTA**2\n",
        "\n",
        "for j in range(0, len(den_sigma_4)):\n",
        "  a = 1 + x[j]**2\n",
        "  den_sigma_4[j] = delta_sq/a\n",
        "\n",
        "sigma_4_sq = np.min(den_sigma_4)/(1 + 2 * np.sqrt(2))\n",
        "sigma_4 = np.sqrt(sigma_4_sq)\n",
        "print(f'\\sigma_4 = {sigma_4}')\n",
        "  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\\sigma_4 = 0.00039926991272375335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoV6F5YIVSvy",
        "colab_type": "text"
      },
      "source": [
        "#### Computation of approximate error\n",
        "We generate $N = 20$ instances of $\\boldsymbol{\\mu}$ distributed as $\\mathcal{N}(0, \\sigma_4^2\\boldsymbol{I})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoy0efeqVyZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code snippet cannot be run independently. Please run all the cells above \n",
        "# before getting here.\n",
        "from scipy import stats\n",
        "\n",
        "# This is our starting vector\n",
        "mu_0 = np.array([0, 0])\n",
        "cov = np.array([[sigma_4_sq, 0], [0, sigma_4_sq]])\n",
        "mu_samples = []\n",
        "N = 20 # number of samples\n",
        "\n",
        "frozen = stats.multivariate_normal(mu_0, cov)\n",
        "mu_samples = frozen.rvs(10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk06ZE8wJR34",
        "colab_type": "text"
      },
      "source": [
        "For each randomly generated $\\boldsymbol{\\mu}$ compute the exact gradient $\\boldsymbol{g}$. Computation of gradient requires a knowledge of the cost function. Let us choose a commonly used cost function - sum of squares of error terms. Thus, \n",
        "$$\\tag{39}\n",
        "C(F, M) = \\sum_{i = 0}^{36} (F_i - M_i)^2,\n",
        "$$\n",
        "where $F_i = F[i]$ and so on. \n",
        "Note that this is true only for our choice of $6 \\times 6$ binary images. Further, since we chose $T(x) = mu_0 + \\mu_1 x$ in equation (28), <code>M[x_i] = mu[0] + mu[1] * F[x_i]</code>. Equation (39) thus becomes\n",
        "$$\\tag{40}\n",
        "C(F, M) = \\sum_{i=0}^{36} ((1 - \\mu_1)F_i - \\mu_0)^2\n",
        "$$\n",
        "and its gradient is\n",
        "$$\\tag{41}\n",
        "\\boldsymbol{g} = \\sum_{i=0}^{36} \\begin{pmatrix}-2((1 - \\mu_1)F_i - \\mu_0) \\\\ -2F_i((1 - \\mu_1)F_i - \\mu_0) \\end{pmatrix} = -2\\sum_{i=0}^{36} ((1 - \\mu_1)F_i - \\mu_0)\\begin{pmatrix}1 \\\\ F_i\\end{pmatrix}\n",
        "$$\n",
        "The approximate gradient is\n",
        "$$\\tag{42}\n",
        "\\tilde{\\boldsymbol{g}} = -2\\sum_{i \\in S} ((1 - \\mu_1)F_i - \\mu_0)\\begin{pmatrix}1 \\\\ F_i\\end{pmatrix},\n",
        "$$\n",
        "where $S$ is a small sample of the set $\\{0, \\ldots, 35\\}$. (We are counting from zero to make it easy for us to translate this theory into code.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kb_Ge-1OkpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code snippet cannot be run independently. Please run all the cells above \n",
        "# before getting here.\n",
        "import random\n",
        "\n",
        "random.seed(12111842)\n",
        "\n",
        "exact_g = []\n",
        "apprx_g = []\n",
        "error_g = []\n",
        "\n",
        "for i in range(len(mu_samples)):\n",
        "  mu = mu_samples[i]\n",
        "  g = np.zeros(2)\n",
        "  for j in range(len(x)):\n",
        "    # Recall that the data for the fixed image is in the array x\n",
        "    X = np.array([1, x[j]]) \n",
        "    coeff = -2 * ((1 - mu[1]) * x[j] - mu[0])\n",
        "    g = g + coeff * X\n",
        "\n",
        "  exact_g.append(g)\n",
        "\n",
        "for i in range(len(mu_samples)):\n",
        "  mu = mu_samples[i]\n",
        "  g = np.zeros(2)\n",
        "  # Select a new set of pixels for computation of each approximate gradient\n",
        "  S = random.sample(range(0, 36), N)\n",
        "  for j in range(len(S)):\n",
        "    # Recall that the data for the fixed image is in the array x\n",
        "    X = np.array([1, x[j]]) \n",
        "    coeff = -2 * ((1 - mu[1]) * x[j] - mu[0])\n",
        "    g = g + coeff * X\n",
        "\n",
        "  apprx_g.append(g)\n",
        "\n",
        "for i in range(len(mu_samples)):\n",
        "  error_g.append(exact_g[i] - apprx_g[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87O259y5yNdy",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKHbndEdV65E",
        "colab_type": "text"
      },
      "source": [
        "Compute $\\sigma_1^2$ as\n",
        "$$\\tag{43}\n",
        "\\sigma_1^2 = \\frac{1}{\\mathrm{tr}(\\boldsymbol{C})}\\frac{1}{N}\\sum_{n=1}^N||\\boldsymbol{g}_n||^2.\n",
        "$$\n",
        "Recall the definition of $\\boldsymbol{C}$ in equation (33). We compute $\\sigma_1^2$ in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlENoFZXW479",
        "colab_type": "code",
        "outputId": "98744752-9a72-42fb-b40e-587580f858f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# This code snippet cannot be run independently. Please run all the cells above \n",
        "# before getting here.\n",
        "\n",
        "trace_C = C[0, 0] + C[1, 1]\n",
        "\n",
        "s = 0 \n",
        "for i in range(len(exact_g)):\n",
        "  g_n = exact_g[i]\n",
        "  s = s + g_n[0]**2 + g_n[1]**2\n",
        "\n",
        "sigma_1_sq = 1/(N * trace_C)\n",
        "print(f'sigma_1_sq = {sigma_1_sq}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sigma_1_sq = 1.3729139335509656e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_UvzyryYRVU",
        "colab_type": "text"
      },
      "source": [
        "Compute $a_{MAX}$ using the relation\n",
        "$$\\tag{44}\n",
        "a_{MAX} = \\frac{A\\delta}{\\sigma_1}\\min_{x_j \\in \\Omega_F^\\prime}\\left[\\mathrm{tr}(\\boldsymbol{J}_j\\boldsymbol{C}\\boldsymbol{J}_j^T) + 2\\sqrt{2}||\\boldsymbol{J}_j\\boldsymbol{C}\\boldsymbol{J}_j^T||_F\\right]^{-1/2}\n",
        "$$\n",
        "Recall that we had chosen $\\delta = 0.1$ in equation (30). $A$ is another free parameter. Let us choose\n",
        "$$\\tag{45}\n",
        "A = 1.\n",
        "$$\n",
        "We computed the matrix $\\boldsymbol{C}$ in equation (33) and $\\boldsymbol{J}_j$ in equation (30). Based on these we can calculate\n",
        "$$\\tag{46}\n",
        "\\boldsymbol{J}_j\\boldsymbol{C}\\boldsymbol{J}_j^T = 1 + 3.64 \\times 10^3 \\times x_j^4\n",
        "$$\n",
        "Since this is just a number, it is the same as its trace and Frobenius norm. Equation (44) then becomes\n",
        "$$\\tag{47}\n",
        "a_{MAX} = \\frac{A\\delta}{\\sigma_1}\\min_{x_j \\in \\Omega_F^\\prime}\\frac{1}{\\sqrt{(1 + 2\\sqrt{2})(1 + 3.64 \\times 10^3 \\times x_j^4)}}\n",
        "$$\n",
        "The code snippet below computes $a_{MAX}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0epd1BYmCXW",
        "colab_type": "code",
        "outputId": "dedea741-a8d1-4de7-e4fe-7e36c28b32d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# This code snippet cannot be run independently. Please run all the cells above \n",
        "# before getting here.\n",
        "\n",
        "all_a = np.zeros(len(x))\n",
        "sigma_1 = np.sqrt(sigma_1_sq)\n",
        "coeff = 0.1 / sigma_1\n",
        "\n",
        "for i in range(len(x)):\n",
        "  a = (1 + 2 * np.sqrt(2)) * (1 + 3.64 * 1E3 * x[i]**4)\n",
        "  all_a[i] = coeff/np.sqrt(a)\n",
        "\n",
        "a_max = np.min(all_a)\n",
        "print(f'a_max = {a_max}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a_max = 1.3953961495741212e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nkoDETOnfZE",
        "colab_type": "text"
      },
      "source": [
        "Compute $\\eta$ and $a$ using the equations\n",
        "$$\\tag{48}\n",
        "\\eta = \\frac{\\mathbb{E}||\\boldsymbol{g}||^2}{\\mathbb{E}||\\boldsymbol{g}||^2 + \\mathbb{E}||\\tilde{\\boldsymbol{g}}||^2}\n",
        "$$\n",
        "and\n",
        "$$\\tag{48}\n",
        "a = a_{MAX} \\cdot \\eta.\n",
        "$$\n",
        "Also\n",
        "$$\\tag{49}\n",
        "f_{min} = \\eta - 1.\n",
        "$$\n",
        "We compute these quantities now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu-N053roePs",
        "colab_type": "code",
        "outputId": "6773a492-0e0a-4fa8-fa96-6bfa9bf33831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# This code snippet cannot be run independently. Please run all the cells above \n",
        "# before getting here.\n",
        "s1 = 0\n",
        "s2 = 0\n",
        "for i in range(len(exact_g)):\n",
        "  g = exact_g[i]\n",
        "  h = apprx_g[i]\n",
        "\n",
        "  s1 = s1 + (g[0]**2 + g[1]**2)\n",
        "  s2 = s2 + (h[0]**2 + h[1]**2)\n",
        "\n",
        "eta = s1/(s1 + s2)\n",
        "a = a_max * eta\n",
        "f_min = eta - 1\n",
        "\n",
        "print(f'eta = {eta}')\n",
        "print(f'a = {a}')\n",
        "print(f'f_min = {f_min}')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eta = 0.7999973773111878\n",
            "a = 1.1163132599694269e-05\n",
            "f_min = -0.2000026226888122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLzM64Bst052",
        "colab_type": "text"
      },
      "source": [
        "The quantity $\\omega$ is computed as\n",
        "$$\\tag{50}\n",
        "\\omega = \\zeta \\sqrt{\\mathrm{Var}(\\boldsymbol{\\epsilon}_k\\boldsymbol{\\epsilon}_{k-1})}.\n",
        "$$\n",
        "We compute it in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUvRIWubuPWQ",
        "colab_type": "code",
        "outputId": "491c405f-86e8-46a5-d5d9-cdcb30ad1bf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# This code snippet cannot be run independently. Please run all the cells above \n",
        "# before getting here.\n",
        "\n",
        "err_prod = np.zeros(len(error_g) - 1)\n",
        "zeta = 0.1\n",
        "\n",
        "for i in range(1, len(error_g)):\n",
        "  e1 = error_g[i]\n",
        "  e2 = error_g[i - 1]\n",
        "  err_prod[i - 1] = np.dot(e1, e2)\n",
        "\n",
        "print(err_prod)\n",
        "\n",
        "omega = zeta * np.std(err_prod)\n",
        "print(f'omega = {omega}')\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.71862149e+10 1.71959182e+10 1.71800672e+10 1.71756754e+10\n",
            " 1.71758954e+10 1.71807175e+10 1.71735969e+10 1.71727391e+10\n",
            " 1.71883437e+10]\n",
            "omega = 731878.7214182274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqdvLU2wyQPW",
        "colab_type": "text"
      },
      "source": [
        "We are now in a position to compute the function $f$ defined by equation (11). We also can write\n",
        "$$\\tag{51}\n",
        "\\gamma(x) = \\frac{a}{x + 1}\n",
        "$$\n",
        "because of our choice of $A = 1$ and $\\alpha = 1$. We now have all the information needed to run the algorithm described by equations (4), (5) and (6).\n",
        "This allows us to implement the algorithm in then ext cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEJXUtAy8xo_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40853149-4e0e-4e15-d44a-b71d0542cf9e"
      },
      "source": [
        "def our_sigmoid(x):\n",
        "  # Recall that f_max = 1\n",
        "  return f_min + (1 - f_min)/(1 - np.exp(-x/omega)/f_min)\n",
        "\n",
        "def calc_t(prev_t, g_curr, g_prev):\n",
        "  arg = prev_t + our_sigmoid(np.dot(g_curr, g_prev))\n",
        "  if arg > 0:\n",
        "    return arg\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "# We re-initialize the image data\n",
        "f = np.zeros(36)\n",
        "f[14] = f[15] = f[20] = f[21] =  f[35] = 128\n",
        "m = np.zeros(36)\n",
        "m[7] = m[8] = m[13] = m[14] = m[28] = 128\n",
        "# 'Difference' between fixed and moving images.\n",
        "x = f - m\n",
        "\n",
        "# Parameters of the algorithm\n",
        "MAX_ITER = 100 # Maximum number of iterations.\n",
        "n_iter = 0     # Current iteration.\n",
        "\n",
        "# Temporary variables\n",
        "g_prev = np.array([1, 1]) # previous gradient\n",
        "mu_prev = np.zeros(2)     # previous parameters\n",
        "prev_t = 0                # previous 'time'\n",
        "\n",
        "while n_iter < MAX_ITER:\n",
        "  n_iter = n_iter + 1\n",
        "  m = mu_prev[0] + mu_prev[1] * f\n",
        "  \n",
        "  # Select a new set of pixels for computation of each approximate gradient\n",
        "  S = random.sample(range(0, 36), 15)\n",
        "  g = np.zeros(2)\n",
        "  for j in range(len(S)):\n",
        "    # Recall that the data for the fixed image is in the array x\n",
        "    X = np.array([1, x[j]]) \n",
        "    coeff = -2 * ((1 - mu[1]) * x[j] - mu[0])    \n",
        "    g = g + coeff * X\n",
        "  \n",
        "  curr_t = calc_t(prev_t, g, g_prev)  \n",
        "  gamma = a_max/(curr_t + 1) # Recall that A = 1, alpha = 1\n",
        "\n",
        "  mu_prev = mu_prev + gamma * g\n",
        "  g_prev = g\n",
        "  prev_t = curr_t\n",
        "  \n",
        "print(f'mu = {mu_prev}')\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mu = [  0.1049243 -13.4303987]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}