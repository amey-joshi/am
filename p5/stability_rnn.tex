\documentclass{article}
\usepackage{amsmath, amssymb, amsfonts, amsthm}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\smax}{softmax}
\numberwithin{equation}{section}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem*{rem}{Remark}
\begin{document}
\title{Stability Analysis of Recurrent Neural Networks \\
    using Ordinary Differential Equations}
\author{Author's name}
\maketitle
\section{Introduction}\label{s1}
Recurrent neural networks feed their output to their input. This feature
makes them look like algorithms to get numerical solutions of ordinary
differential equations. It is well known that training neural networks is
rife with vanishing and exploding gradients\cite{gereon2018hands}. This
phenomenon is closely related to the behaviour of the numerical algorithms
to solve ordinary differential equations, a topic that has been studied
intensively for several decades. We use this analogy to bank upon the 
vast research about ordinary differential equations to derive criteria 
for the stability of recurrent neural networks.

\section{Notation}\label{s2}
We follow the contemporary mathematical practice of not using separate
notation for vectors and scalars. Instead, we define every symbol. Whenver
a symbol stands for a vector, it denotes a column vector. $\mathbb{R}$ is 
the set of real numbers. $\mathbb{Z}$ is the set of integers. If $z$ is
a complex number then $\Re(z)$ denotes its real part and $\Im(z)$ its
imaginary part.

\section{Ordinary Differential Equations of the first order}\label{s3}
An ordinary differential equation, abbreviated as ODE, of the first order 
is an equation of the form
\begin{equation}\label{s3e1}
\frac{dy}{dx} = f(x, y),
\end{equation}
where $x \in \mathbb{R}$, $y$ is the real-valued unknown function, $f: 
\mathbb{R}^2 \mapsto \mathbb{R}$. Note that $f$ depends on both $x$ and 
$y$. It called a first order ODE because the highest order derivative
of the unknown function $y$ with respect to $x$ is of the first order.
This equation is usually accompanied with the initial value of $y$ at a 
certain $x$, say $x = x_0$. Equation \eqref{s3e1} and $y(x_0) = y_0$ is
called an \emph{inital value problem}.

The oldest method to solve equation \eqref{s3e1} numerically was devised 
by Euler in the eighteenth century. It follows straight from the definition
of the first derivative of a function. If we know the value of $y$ at $x_0$,
we can approximate the value of $y$ at a neighbouring point $x_0 + h$ as
\begin{equation}\label{s3e2}
y(x_0 + h) = y(x_0) + h\left[\frac{dy}{dx}\right]_{x_0}.
\end{equation}
The square bracket around the derivative and the subscript $x_0$ to the
right bracket indicates that the derivative is to be evaluated at $x = x_0$.
The value of the derivative of $y$ with respect to $x$ is given by
the differential equation itself. Thus,
\begin{equation}\label{s3e3}
y(x_0 + h) = y(x_0) + h f(x_0, y(x_0)).
\end{equation}
In general, $n$ steps beyond the first, one can write
\begin{equation}\label{s3e4}
y(x_n + h) = y(x_{n}) + h f(x_{n}, y(x_{n})).
\end{equation}
Euler's algorithm is not the best numerical algorithm to solve ODE's but
it provides the germ for the rest.

\section{RNN with a Single Layer}\label{s4}
Recurrent neural networks, abbreviated as RNN, feed a part of their output
to their input. The output of a step $n$ thus depends on both the input
at that step and the output of the previous step. This makes them similar
to the ordinary differential equations described in the previous section.
Suppose we have a single layer neural network that is updated using the
following rule:
\begin{equation}\label{s4e1}
y_{n+1} = \sigma(w_x x_{n} + w_y y_n),
\end{equation}
where $w_x$ and $w_y$ are weights and $\sigma$ is an activation function. 
If we now identify the correspondence
\begin{eqnarray}
y_{n+1} &\rightarrow& y(x_n + h) \\
y_{n} &\rightarrow& y(x_n) \\
\sigma(w_x x_n + w_y y_n) &\rightarrow& y(x_n) + hf(x_n, y(x_n))
\end{eqnarray}
then we can spot a resemblance between Euler's algorithm and an
RNN. The idea that an RNN corresponds to an ODE and therefore the stability
of an RNN can be studied using the stability of the underlying ODE is the
topic of this article.

We need not interpret equation \eqref{s3e1} as a scalar equation. We can
as well let $y$ and $f$ be vector functions. That is, $y:\mathbb{R}^p
\mapsto \mathbb{R}^q$, $x \in \mathbb{R}^p$ and $f: \mathbb{R}^{p+q}
\mapsto \mathbb{R}^q$. In this case, equation \eqref{s3e4} becomes
\begin{equation}\label{s4e5}
y(x_n + h) = y(x_n) + \nabla y(x_n)\cdot h.
\end{equation}
Here
\begin{equation}\label{s4e6}
\nabla y = \begin{pmatrix} y_{1, x_1} & \ldots & y_{1, x_p} \\
	   \vdots \\
	   y_{q, x_1} & \ldots & y_{q, x_p}
	   \end{pmatrix}
\end{equation}
is an $q \times p$ matrix and $h \in \mathbb{R}^p$ so that their product
$\nabla y(x_n) \cdot h$ is in $\mathbb{R}^q$. The symbol $y_{i, x_j}$ 
stands for the partial derivative of the $i$th component of $y$, denoted by
$y_i$ with respect to $x_j$. Thus,
\[
    y_{i, x_j} = \frac{\partial y_i}{\partial x_j}.
\]
An RNN corresponding to this equation is
\begin{equation}\label{s4e7}
y_{n+1} = \sigma\left(W_x x_n + W_y y_n\right),
\end{equation}
where $y_n$ and $y_{n+1}$ are $q$-dimensional vectors, $x_n \in \mathbb{R}
^p$, $W_x$ is an $q \times p$ weight matrix for the input and $W_y$ is an
$q \times q$ weight matrix for the output.

\section{Higher order ODE}\label{s5}
We can interpret the vector form of equation \eqref{s3e1} as a system of 
first order ODE
\begin{eqnarray*}
\frac{dy_1}{dx} &=& f_1(x, y_1, \ldots, y_k) \\
\vdots \\
\frac{dy_k}{dx} &=& f_k(x, y_1, \ldots, y_k) \\
\end{eqnarray*}
Its equivalent vector form is
\begin{equation}\label{s5e1}
\frac{d}{dx}\begin{pmatrix}y_1 \\ \vdots \\ y_k\end{pmatrix} = 
\begin{pmatrix}f_1 \\ \vdots \\ f_k\end{pmatrix}.
\end{equation}
A system of first order ODE is interesting because it is equivalent to
a higher order differential equation. For example, one can write the $k$th
order ODE
\begin{equation}\label{s5e2}
\frac{d^ky}{dx^k} = f\left(x, y, \frac{dy}{dx}, \ldots, 
	\frac{d^{k-1}y}{dx^{k-1}}\right)
\end{equation}
as the system
\begin{eqnarray*}
\frac{dy_{k-1}}{dx} &=& f(x, y, y_1, \ldots, y_{k-1}) \\
\frac{dy_{k-2}}{dx} &=& y_{k-1} \\
\vdots \\
\frac{dy}{dx} &=& y_1
\end{eqnarray*}
If this equation is also accompanied by an initial value then we can solve
it numerically using equations \eqref{s4e5}, \eqref{s4e6} and \eqref{s4e7}.

\section{Runge-Kutta methods}\label{s6}
Runge-Kutta methods are a collection of numerical algorithms developed by
the German mathematicians Carl Runge and Martin Kutta in the late 
nineteenth century\cite{lambert1991numerical}. We will describe the most
commonly used among them, called the `RK4' method. If $y: \mathbb{R}
\mapsto \mathbb{R}$ is the unknown function that obeys the ODE
\begin{equation}\label{s6e1}
\frac{dy}{dt} = f(t, y), y(t_0) = y_0
\end{equation}
then one can get the value oy $y$ at $t_n + h$ using the formula
\begin{equation}\label{s6e2}
y(t_n + h) = y(t_n) + \frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4).
\end{equation}
The constants $k_1, \ldots, k_4$ are given by
\begin{eqnarray}
k_1 &=& f(t_n, y(t_n)) \label{s6e3} \\
k_2 &=& f\left(t_n + \frac{h}{2}, y(t_n) + h\frac{k_1}{2}\right) 
 \label{s6e4} \\
k_3 &=& f\left(t_n + \frac{h}{2}, y(t_n) + h\frac{k_2}{2}\right) 
 \label{s6e5} \\
k_4 &=& f(t_n + h, y_n + hk_3). \label{s6e6}
\end{eqnarray}
Let us study these equations carefully. The value of $y$ at $t_n + h$ 
depends on $y(t_n)$ and four other numbers $k_1, \ldots, k_4$. Each of
these is the value of the derivative of $y$ at different points. Of these,
$k_1$ depends on $y(t_n)$ and $t_n$, $k_2$ on $y(t_n)$, $t_n$ and $k_1$,
$k_3$ on $y(t_n), t_n$ and $k_2$ and $k_4$ on $y(t_n), t_n$ and $k_3$. If
one were to implement this algorithm, every iteration of it would consist
of using the output of the previous iteration and a calculation of four
constants, one after another, to get the value of the current iteration.
This is indeed like a recurrent neural network with four hidden layers.

Runge-Kutta methods perform much better then Euler methods when $f$ is 
a non-linear function of $x$. The number of intermediate values, $4$ in 
this case, is called the \emph{the number of stages} of the algorithm.

\section{RNN with Hidden Layers}\label{s7}
Let us try to interpret the `RK4' algorithm as a neural network. It is 
clearly a recurrent neural network because the output at `time' $t_n + h$
depends on that at `time' $t_n$. If we write $y(t_n) = y_n$ then we can
say that $y_{n+1}$ depends on $y_n$. However, it does not depend on $y_n$
alone but on four other numbers which must be computed sequentially. $k_1$
is computed from $t_n$ and $y_n$. The rest from $t_n$, $y_n$ and the 
`previous' $k$. We can thus construct a neural network with four hidden 
layers, each computing $k_1$ to $k_4$ and the output later combining
all of them to get the final result. We call a recurrent neural network
implementing an algorithm to solve an ordinary differential equation an
ODERNN.

We now define a general ODERNN\cite{niu2019recurrent}.
\begin{defn}\label{s7d1}
An ODE-RNN of $n$-th order in non-linearity and $t$-th order in gradient,
called $(n,t)$-ODERNN, where $n, t \ge 1$ are integers is described by
the following update rule.
\begin{eqnarray}
K_1 &=& \sigma_1(W_1Y_l + b_1) \label{s7e1} \\
K_q &=& \gamma_q K_{q-1} + \kappa_1\sigma_q\left(
 W_qY_{l-t_{q-1}}+b_q+h\sum_{k=1}^{q-1}\alpha_{q,k}K_k\right)\label{s7e2}\\
Y_{l+1} &=& \gamma_{n+1}Y_l + \kappa_{n+1}\sigma_{q+1}\left(
 W_{n+1}Y_{l-t_{n-1}}+b_{n+1}+h\sum_{k=1}^n\beta_kK_k
 \right)\label{s7e3},
\end{eqnarray}
where
\begin{enumerate}
\item $q = 1, \ldots, n$. This explains the $n$ in the $(n,t)$-ODERNN.
\item The right hand side of equations \eqref{s7e1} to \eqref{s7e3} 
depend on $Y_l$ to $Y_{l - t_{n-1}}$, a total of $n$ previous values.
Here each $t_k = \lfloor tk/n\rfloor$. Because $t_k$ for each hidden layer
is defined in terms of $t$, we also specify it in the name of the ODE. 
This explain $t$ in $(n, t)$-ODERNN.
\item $Y_l \in \mathbb{R}^s$, $l \in \mathbb{Z}$;
\item $W_1,\ldots, W_{n+1}$ are $p \times s$ weight matrices; 
\item $b_1,\ldots, b_{n+1} \in \mathbb{R}^p$ are bias vectors; 
\item $\alpha_{q, k}$, for $q = 2, \ldots, n$ and $k = 1, \ldots, n - 1$
are $p \times p$ matrices. So are $\gamma_2, \ldots, \gamma_{n}, \kappa_2, 
\ldots, \kappa_{n}$;
\item $\gamma_{n+1}$ is an $s \times s$ matrix and $\kappa_{n+1}$ is a 
$s \times p$ matrix.
\item $h \in \mathbb{R}$ is a constant and
\item $\sigma_1, \ldots, \sigma_{q+1}$ are activation function defined as
\[
\sigma_i\begin{pmatrix}x_1 \\ \vdots \\ x_p\end{pmatrix} = 
\begin{pmatrix}\sigma_i(x_1) \\ \vdots \\ \sigma_i(x_p)\end{pmatrix},
i = 1, \ldots, q+1.
\]
\end{enumerate}
\end{defn}
\noindent Before proceeding, a few remarks are in order.
\begin{rem}
There is probably a typo in equation (3) of section 2.1 Niu and other's
paper\cite{niu2019recurrent}. I believe that the range of the sum should
be $1$ to $q - 1$ as given here instead of $1$ to $n$ as mentioned in the
paper.
\end{rem}
\begin{rem}
$K_1, \ldots, K_n$ are all $p$-dimensional vectors. That is, they 
are all members of $\mathbb{R}^p$.
\end{rem}
\begin{rem}
We can readily confirm that the arguments to the activation functions
$\sigma_1, \ldots, \sigma_{q+1}$ are all $p$-dimensional vectors.
\end{rem}
\begin{rem}
The number of hidden layers is $n$. The RNN thus has $n + 2$ layers. 
\end{rem}

\noindent We also define
\begin{defn}\label{s7d2}
The Burrage-Butcher tensor corresponding to the $(n,t)$-ODERNN of 
definition \ref{s7d1} is
\begin{equation}\label{s7e4}
Q_{i, j} = \beta_i\alpha_{i, j} + \beta_j\alpha_{j, i} - \beta_i\beta_j^T.
\end{equation}
\end{defn}

\noindent Let us consider a few examples of $(n,t)$-ODERNN.
\begin{enumerate}
\item $(2,2)$-ODERNN. It has the composition rules
\begin{eqnarray*}
K_1 &=& \sigma_1(W_1 Y_l + b_1) \\
K_2 &=& \gamma_2K_1 + \kappa_2\sigma_2(W_2Y_{l-1}+b_2+h\alpha_{2,1}K_1) \\
Y_{l+1} &=& \gamma_3Y_l + \kappa_3\sigma_3\left(
 W_3 Y_{l-1} + b_3 + h(\beta_1K_1 + \beta_2K_2)\right)
\end{eqnarray*}
\item $(4, 2)$-ODERNN. It has $4$ hidden layers and it uses $2$ previous
inputs. It has the composition rules
\begin{eqnarray*}
K_1 &=& \sigma_1(W_1 Y_l + b_1) \\
K_2 &=& \gamma_2K_1 + \kappa_2\sigma_2(W_2Y_{l-1}+b_2+h\alpha_{2,1}K_1) \\
K_3 &=& \gamma_3K_2 + \kappa_3\sigma_3\left(W_3Y_{l-1} + b_3 + 
    h\sum_{k=1}^{2}\alpha_{3, k}K_k\right) \\
K_4 &=& \gamma_4K_2 + \kappa_4\sigma_4\left(W_4Y_{l-2} + b_4 + 
    h\sum_{k=1}^{3}\alpha_{4, k}K_k\right) \\
Y_{l+1} &=& \gamma_5Y_l + \kappa_5\sigma_5\left(W_5Y_2 + b_5 + 
    h\sum_{k=1}^{4}\alpha_{5, k}K_k\right) 
\end{eqnarray*}
\end{enumerate}

\section{Gated Recurrent Unit as ODERNN}\label{s8}
Consider a single layer of a Gated Recurrent Unit (GRU). Its composition
rule is\cite{gereon2018hands}
\begin{eqnarray}
\begin{pmatrix} r_t \\ z_t \end{pmatrix} &=&
\sigma
\begin{pmatrix}
    W_r X_t + U_r Y_{t-1} + b_z \\ 
    W_rX_t + U_rY_{t-1} + b_r
\end{pmatrix} \label{s8e1} \\
Y_t &=& (1 - z_t) \circ Y_{t-1} + 
 z_t \circ \tanh(W_hX_t + U_h(r_t \circ Y_{t-1} + b_h)) \label{s8e2}
\end{eqnarray}
where $\circ$ denotes the element-wise product, also called the Hadamard
product of vectors. There are some differences between this composition
rule and the one mentioned in the paper\cite{niu2019recurrent} by Niu and 
others. We prefer these because the equations they are more general than
the equations (22) and (23) of Niu's paper.

We now introduce the functions
\begin{eqnarray}
\sigma_t^\prime(X) &=& r_t \circ X \label{s8e3} \\
\sigma_t^{\prime\prime}(X) &=& z_t \circ \tanh(X). \label{s8e4}
\end{eqnarray}
The functions $\sigma^\prime$ and $\sigma^{\prime\prime}$ allow us to 
write the composition rule of equation \eqref{s8e2} as
\begin{equation}\label{s8e5}
Y_t = (1 - z_t) \circ Y_{t-1} + \sigma_t^{\prime\prime}\left(
	W_tY_{t-1} + U_h(\sigma_t^\prime(Y_{t-1}) + b_h)\right).
\end{equation}
If $x$ and $y$ are vectors then we can always express their Hadamard 
product as a matrix operation
\begin{equation}\label{s8e6}
x \circ y = D[x]y,
\end{equation}
where $D[x]$ is a diagonal matrix whose diagonal entries as same as $x$.
Using this transformation, we observe that equation \eqref{s8e5} has the
same structure as \eqref{s7e3}. Further, there are two hidden layers, 
one each for $r_t$ and $z_t$. This makes the GRU of equations \eqref{s8e1}
and \eqref{s8e2} a $(2, 1)$-ODERNN.

\section{Stability analysis}\label{s9}
RNNs frequently have a problem of `exploding gradients'
\cite{gereon2018hands}. In the context of ODEs this is called an 
instability. Because of a correspondence between an RNN and an ODE, we 
can study the problem of exploding gradients in the former as the 
instability in the latter. In this section we will review certain notions
of stability of ODE.

Consider the initial value problem of equation \eqref{s3e1}. The general
$k$-step method to get its approximate numerical solution is defined by
the formula\cite{dahlquist1963special}
\begin{equation}\label{s9e1}
\sum_{i=0}^k \alpha_i x_{n+i} = h\sum_{i=0}^k \beta_i f_{n+i},
\end{equation}
where $\alpha_i, \beta_i$ are real constants.
\begin{defn}\label{s9d1}
The $k$-step method defined above is said to be A-stable
\cite{dahlquist1963special} if all solutions of \eqref{s9e1} tend to zero 
as $n \rightarrow \infty$ when the method is applied to the equation
\[
    \frac{dy}{dx} = qx
\]
where $q \in \mathbb{C}$ with $\Re(q) < 0$ and a fixed $h$ in equation
\eqref{s9d1}. Here $n$ is the number of steps in the numerical algorithm.
\end{defn}

In order to introduce the idea of B-stability and BN-stability, we need
the definition of contractive solutions\cite{lambert1991numerical}.
\begin{defn}\label{s9d2}
Let $y_1$ and $y_2$ be any two solutions of \eqref{s3e1} satisfying the
initial conditions $y_1(a) = \eta_1, y_2(a) = \eta_2, \eta_1 \ne \eta_2$.
Then if $||y_1(x_2) - y_2(x_2)|| \le ||y_1(x_1) - y_2(x_1)||$ for all
$a \le x_1 \le x_2 \le b$ then the solutions are said to be contractive
in $[a, b]$.
\end{defn}
Contractive solutions get closer to each other as one gets away from
the initial point.

\begin{defn}\label{s9d3}
An ODE of the form \eqref{s3e1} is said to be autonomous if the right hand
side is independent of $x$.
\end{defn}

We are now in a position to define B and BN stability
\cite{lambert1991numerical}.
\begin{defn}\label{s9d4}
If a Runge-Kutta method is applied to an autonomous system of ODE such that
\[
    \langle f(x, y_1) - f(x, y_2), y_1 - y_2\rangle \le 0
\]
generates contractive numerical solutions then the method is said to be
B-stable. If the same is true for a non-autonomous system then the method
is said to be $BN$-stable.
\end{defn}
\begin{rem}
$\langle v_1, v_2 \rangle$ is just the inner product of the two vectors.
\end{rem}

It is also mentioned in Lambert's book \cite{lambert1991numerical} that
\begin{thm}\label{s9t1}
Runge-Kutta method is $BN$-stable if the matrices $\beta_n$ (equation
\eqref{s7e3}) and the Burrage-Butcher tensor are positive semi-definite.
\end{thm}

The GRU described in section \ref{s8} has $\alpha_{i, j} = 0$ because the
hidden layers are not connected to each other. The Burrage-Butcher tensor 
is thus $-\beta_1 \beta_1^T$, which is \emph{not} positive semidefinite 
unless $\beta_1 = 0$.  Thus the GRU is not a stable algorithm, an 
observation also corroborated by elsewhere\cite{kanai2017preventing}.

\section{GRU with an Attention Layer}\label{s10}
We will now analyze GRU with attention layer used in three separate
architectures. 
\subsection{Architecture proposed Ran and others} 
We assume that the architecture of the network is same as shown in figure 4 
of the paper \cite{ran2019lstm} by Ran and others, except that the 
Long Short-Term Model (LTSM) is replaced with a GRU. There are $n$ such 
GRU's, each with an output $h^i_l \in \mathbb{R}^s$. The combined output 
of all of them is the $s
\times n$ matrix $Y_l$ defined by
\begin{equation}\label{s10e1}
H_l = [h^1_l, \ldots, h^n_l].
\end{equation}
The matrix $H_l$ is scaled with an $n \times 1$  weight matrix $W_H$. 
Similarly, if $A$ is an $s \times n$ embedding matrix and $W_v$ is the
corresponding weight matrix, then
\begin{equation}\label{s10e2}
M_l = \tanh\begin{pmatrix} H_lW_H \\ AW_v \end{pmatrix}
\end{equation}
is a member of $\mathbb{R}^{2s}$. If $W= [w_1, \ldots, w_n]$ is a $(2s) 
\times n$ weight matrix then
\begin{equation}\label{s10e3}
a_i = \frac{\exp(w_i^T M_l)}{\sum_{j=1}^n \exp(w_j^T M_l)}
\end{equation}
are the $n$ components of the vector $a = (a_1, \ldots, a_n)
\in \mathbb{R}^n$. Finally, the product
\begin{equation}\label{s10e4}
Y_l = H_l a
\end{equation}
is a member of $\mathbb{R}^s$. Note that the output of the individual
GRUs is also an $s$-dimensional vector.

Equations \eqref{s10e1} to \eqref{s10e4} where $Y_l$ is gived by 
equations \eqref{s8e1} to \eqref{s8e5} defined GRU with an attention layer.
We will analyse the stability of this architecture in this section.

Equation \eqref{s10e4} tells that $Y_l$ depends $H_l$ but not the $H_{l-1}$.
Thus, it is not a recurrence relation. $Y_l$ is not computed from the
`present' $H_l$, not a `previous' one. Therefore, this attention layer 
does not play a role in determining the stability of the network in the
architecture shown in figure 4 of the paper \cite{ran2019lstm} by 
Ran and others.

\subsection{Architecture proposed by Zang and others}
We now consider the model proposed by Zang and others\cite{zhang2017gru}.
The model is described by equations (11) to (17) in their paper. We
reproduce them for an easy reference.
\begin{eqnarray}
z^\prime_t &=& \sigma(W_{yz}Ey_{t-1}+U_{sz}s_{t-1}+C_{cz}c_t)\label{s10e5}\\
r^\prime_t &=& \sigma(W_{yr}Ey_{t-1}+U_{sr}s_{t-1}+C_{cr}c_t)\label{s10e6}\\
\tilde{s}_t &=& \tanh\left(W_{ys}Ey_{t-1} + U_{rs}(r^\prime_t \circ
	    s_{t-1} + C_{cs}c_t\right) \label{s10e7} \\
s_t &=& (1 - z^\prime_t)\circ s_{t-1} + z^\prime_t \circ \tilde{s}_t
    \label{s10e8} \\
e_{ti} &=& \nu^T_{\text{att}}\tanh(W_{\text{att}}s_{t-1} +
    U_\text{att}a_i \label{s10e9} \\
\alpha_{ti} &=& 
    \frac{\exp(e_{ti})}{\sum_{k=1}^L\exp(e_{tk})}\label{s10e10}\\
c_t &=& \sum_{i=1}^L \alpha_{ti}a_i. \label{s10e11}
\end{eqnarray}
In these equations the lower case symbols are either vectors or scalars;
the upper case symbols are matrices. In particular:
\begin{itemize}
\item $y_i \in \mathbb{R}^K$;
\item $a_i \in \mathbb{R}^D$;
\item $s_i, \tilde{s}_i, z_i, r_t \in \mathbb{R}^n$;
\item $W_{yz}, W_{yr}, W_{ys}, U_{sz}, U_{sr}, U_{rs}, C_{cz}, C_{cr}, 
C_{cs}$ are weight matrices of appropriate dimensions. $E$ is an embedding
matrix;
\item If $n^\prime$ is the attention dimension then $\nu_{\text{att}}^{n
^\prime}$, $W_\text{att}$ is an $n \times n^\prime$ matrix and $U_a$ is an 
$n^\prime \times D$ matrix.
\end{itemize}
The attention layer determines the vector $c_t$, which is analogous to
the bias vectors $b_q$, $q = 1, \ldots, n + 1$ in equations \eqref{s7e1}
to \eqref{s7e3} in definition \ref{s7d1}. The bias vectors \emph{do not}
contribute to the Burrage-Butcher tensor defined in \eqref{s7e4}. Therefore,
we conclude that the attention layer has no influence on the stability of
the underlying network.

\subsection{Architecture proposed by Liu and others}
In this section we consider the neural network used in table-to-text
generation by Liu and others \cite{liu2018table}. The goal of their
network is to generate a sequence
\begin{equation}\label{s10e12}
w^\ast_{1:p} = \argmax_{w_{1:p}}\prod_{t=1}^p\mathrm{P}(w_t | w_{0:t-1}, 
	R_{1:n}),
\end{equation}
where $w_i$ are generated tokens and $R_n$ are field value records in a
table. The probability on the right hand side is defined by
\begin{equation}\label{s10e13}
\mathrm{P}(w_t| w_{0:t-1}, R_{1:n}) = \smax(W_s \circ g_t),
\end{equation}
where $W_s$ is a weight matrix and
\begin{equation}\label{s10e14}
g_t = \tanh(W_t[s_t, a_t])
\end{equation}
$s_t$ being the output of an LTSM neural network and $a_t$ being the 
attention vector. It is built as
\begin{equation}\label{s10e15}
a_t = \sum_{i=1}^L \alpha_{t_i}h_i,
\end{equation}
where
\begin{equation}\label{s10e16}
\alpha_{t_i} = \frac{\exp(g(s_t, h_i))}{\sum_{j=1}^N\exp(g(s_t, h_j))},
\end{equation}
and
\begin{equation}\label{s10e17}
g(s_t, h_i) = \tanh(W_p h_i) \circ \tanh(W_q s_t).
\end{equation}
Here $s_t$ is the output of the LTSM at time $t - 1$ and 
\begin{equation}\label{s10e18}
h_i = o_t \circ \tanh(c_t).
\end{equation}
The variables in this equation are boolean vectors of size $n$. They 
are defined as
\begin{equation}\label{s10e19}
\begin{pmatrix}i_t \\ f_t \\ o_t \\ \hat{c}_t \end{pmatrix} = 
\begin{pmatrix}\sigma \\ \sigma \\ \sigma \\ \tanh\end{pmatrix}
W_{4n, 2n}^c\begin{pmatrix}d_t \\ h_{t-1} \end{pmatrix}
\end{equation}
and
\begin{equation}\label{s10e20}
c_t = f_t \circ c_{t-1} + i_t \circ \hat{c}_t.
\end{equation}
Analogous to the case of the architecture proposed by \cite{ran2019lstm}
the attention layer $a_t$ is used solely that the final stage. It is not
fed back to the input and therefore it has no influence on the stability
of the LTSM.

\subsection{Remarks on stability}
Recall theorem \ref{s9t1} that the stability of the Runge-Kutta method 
depends on the whether or not the matrices $\beta_n$ \emph{and} the 
Burrage-Butcher tensor are positive semi-definite. An attention layer will
influence the stability of an algorithm if it influences either $\beta_i$
of $\alpha_{i,j}$ which together form the matrices that determine the
stability. The architectures of Ran et al. and Liu et al. use the 
attention layer only at the last stage. They do not feed the attention
later at time $t$ to get the output at time $t + 1$. On the other hand, 
the architecture of Zang et al. uses the attention layer in bias vectors
which do not determine the matrices that decide the stability of the
algorithm.

The stability of Runge-Kutta methods depends on their ability to generate
contractive solutions in an interval. If a method is not stable it does not
mean that it cannot generate \emph{any} contractive pair of solutions. All
it means is that \emph{not all} pairs are contractive in the sense of 
definition \ref{s9d2}. Existence of one, or a few, solutions to the 
algorithm is not a contradiction to it being unstable. Instability means
that \emph{not all} solutions are stable, and stability means that
\emph{all} solutions are stable. An existence of a few coverging solutions
does not tell us anything about the stability of the algorithm. However, 
an existence of a single unstable solution renders an algorithm unstable.

\bibliographystyle{plain}
\bibliography{stability_rnn}
\end{document}
