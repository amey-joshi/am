\documentclass[11pt]{article}
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{physics, mathtools, graphicx}
\numberwithin{equation}{section}

\newcommand{\opr}{\prime}
\linespread{1.2}
\title{Young's Experiment}
\author{To be added}
\begin{document}
\maketitle
\section{Introduction}
Sir Isaac Newton proposed that light is a stream of tiny particles emitted
from a luminous body. His influence on the world of physicists ensured that
his corpuscular theory of light was favored over the rival wave theory for 
almost two centuries. Thomas Young's double-slit experiment that the most
convincing proof that light behaves like a wave. The experiment consists in
passing light through two slits cut out in a screen and observing the 
illumination on another distant screen. When the slits are wide they create
two separate illuminated regions on the screen. When they are narrow, the
two bright regions fade away and a pattern of alternating dark and bright
fringes appears. Young reasoned that these fringes are a result of constructive
and destructive interference of light waves. Light emerging from narrow slits
diffracts into the geometrical shadow. The interference fringes are produced by
the diffracted waves. Both diffraction and interference are phenomena associated
with waves.

Young's experiment showed that when the slits are wide enough, light indeed
travels in a straight line like Newtonian particles described in Newton's first
law. This is the realm of `geometrical optics'. In this realm, light casts
sharp shadows. However, when the slits are narrow, the shadows are no longer
sharp. A penumbra appears and as the slits narrow down further the penumbra 
evolves into an interference pattern as shown in figure \ref{f1}.

\begin{figure}
\centering
\includegraphics[scale=0.5]{young}
\caption{Young's experiment. Figure from reference \cite{wiki1}}\label{f1}
\end{figure}

Young explained his experiment by proposing that the two slits act as sources
of `secondary waves' which interfere on the screen. The wavelength $\lambda$
of the two sources is identical. When the path difference of the two waves
at a point on the screen is an integral multiple of $\lambda$, the waves
interfere constructively and result in a bright band. On the other hand, when
the path difference is an odd multiple of $\lambda/2$, the waves interfere
destructively, resulting in a dark band.

Most common sources of light produce a range of wavelengths. As a result, the
interference bands produced on the screen are not as sharp as one would expect
were the light monochromatic. They also produce light waves whose phase 
changes randomly at an extremely fast rate. Therefore, if we use two separate
light sources illuminating the two slits, there is no fixed path difference
at any point on the screen and therefore no interference pattern is observed.
In order to overcome this problem, the two slits are always illuminated by 
the same source of light. This ensures that irrespective of the random phase
fluctuations at the source, the path difference is a function of the 
experiment's geometry alone.

If $d$ is the distance between the slits and $D$ is the separation between
the screen that has the slits and the screen on which we observe the 
interference pattern then the distance between two consecutive bright (or
dark) fringes is given by \cite{jenkins1937fundamentals}
\begin{equation}\label{s1e1}
\delta y = \lambda \frac{D}{d},
\end{equation}
where $\lambda$ is the wavelength of light.

Although Young's experiment demonstrated the wave nature of light in 1801, it 
was not clear in at that time what exactly was oscillating. The answer to that
question had to wait six decades until James Clerk Maxwell proposed his 
equations governing the electric and magnetic phenomena. His genius lay in 
adding a `displacement current' term to Ampere's law and realizing that the 
other known laws like Gauss law, Faraday's law, the fact that there are
no magnetic monopoles together can explain all electric and magnetic 
phenomena. An immediate consequence of his equations was his wave equation
which showed the existence of electromagnetic waves. Maxwell proposed that
light is a form of an electromagnetic wave and that gave a theoretical
explanation to Young's experiment.

\section{Maxwell equations}\label{s2}
Maxwell equations, in SI units, are
\begin{eqnarray}
\div\vb{D} &=& \rho_f \label{s2e1} \\
\curl\vb{E} &=& -\pdv{\vb{B}}{t} \label{s2e2} \\
\div\vb{B} &=& 0 \label{s2e3} \\
\curl\vb{H} &=& \vb{J}_f + \pdv{\vb{D}}{t}, \label{s2e4}
\end{eqnarray}
where $\vb{E}$ is the electric field, $\vb{B}$ is the magnetic field,
$\vb{D}$ is the electric displacement and $\vb{H}$ is the auxilary magnetic
vector. $\rho_f$ is the density of free charges and $\vb{J}_f$ is the 
density of free current. Equation \eqref{s2e1} is Gauss' law. Equation
\eqref{s2e2} is Faraday's law. Equation \eqref{s2e3} is the mathematical
expression of the fact that there are no magnetic monopoles. Equation
\eqref{s2e4} is Ampere's law. The term $\partial\vb{D}/\partial t$ in equation
\eqref{s2e4} is called the displacement current. Adding it to Ampere's law
was Maxwell's correction to the law. We will show that this term is critical
in getting the equation of electromagnetic waves.

The electric displacement and the electric field
are related by the relation
\begin{equation}\label{s2e5}
\vb{D} = \epsilon_0\vb{E} + \vb{P},
\end{equation}
where $\epsilon_0$ is the permittivity of free space and $\vb{P}$ is the
polarization of the medium. It is the number of electric dipoles per unit
volume of the material. The volume of the material is chosen to be small at
a macroscopic scale but large enough at a microscopic scale so that it contains
a very large number of molecules of the material. The magnetic field $\vb{B}$
is related to the field $\vb{H}$ by the relation,
\begin{equation}\label{s2e6}
\vb{H} = \mu_0^{-1}(\vb{B} - \vb{M}),
\end{equation}
where $\mu_0$ is the permeability of free space and $\vb{M}$ is the 
magnetization of the medium. It is the number of magnetic dipoles per unit
volume of the material. The volume of the material is chosen in a manner 
similar to the one used to define $\vb{P}$. 

There are no electric or magnetic dipoles in vacuum so that equations
\eqref{s2e5} and \eqref{s2e6} become
\begin{eqnarray}
\vb{D} &=& \epsilon_0\vb{E} \label{s2e7} \\
\vb{H} &=& \mu_0^{-1}\vb{B} \label{s2e8}
\end{eqnarray}
It is also common to write equations \eqref{s2e5} and \eqref{s2e6} as
\begin{eqnarray}
\vb{D} &=& \epsilon\vb{E} \label{s2e9} \\
\vb{H} &=& \mu^{-1}\vb{B}, \label{s2e10}
\end{eqnarray}
where $\epsilon$ and $\mu$ are permittivity and permeability of media. These
equations are applicable to linear, isotropic media. A general form of these
equations is
\begin{eqnarray}
D_i &=& \epsilon_{ij}E_j \label{s2e11} \\
B_i &=& \mu_{ij}H_j \label{s2e12}
\end{eqnarray}
where $\epsilon_{ij}$ is the permittivity tensor and $\mu_{ij}$ is the
permeability tensor of the medium. 

In this article, we will consider electromagnetic waves in vacuum where there 
are no free charges or currents. Therefore $\rho_f = 0$ and $\vb{J}_f= 0$.
Maxwell equations, in this case, simplify to
\begin{eqnarray}
\div\vb{E} &=& 0 \label{s2e13} \\
\curl\vb{E} &=& -\pdv{\vb{B}}{t} \label{s2e14} \\
\div\vb{B} &=& 0 \label{s2e15} \\
\curl\vb{B} &=& \mu_0\epsilon_0\pdv{\vb{E}}{t} \label{s2e16}
\end{eqnarray}
Taking the curl of equation \eqref{s2e14} and using equation \eqref{s2e16}
in the result, we get
\begin{equation}\label{s2e17}
\laplacian\vb{E} = \mu_0\epsilon_0\pdv[2]{\vb{E}}{t}.
\end{equation}
This is the equation of an electromagnetic wave in vacuum. Its speed is
\begin{equation}\label{s2e18}
c = \frac{1}{\sqrt{\epsilon_0\mu_0}}.
\end{equation}
If we take the curl of equation \eqref{s2e16} and use equation \eqref{s2e14},
we get
\begin{equation}\label{s2e19}
\laplacian\vb{B} = \mu_0\epsilon_0\pdv[2]{\vb{B}}{t}.
\end{equation}
Thus, the magnetic field $\vb{B}$ satisfies an identical equation. The key
physical idea governing the wave equations \eqref{s2e17} and \eqref{s2e19}
is that a changing magnetic field produces an electric field (Faraday's law
of equation \eqref{s2e14}) and a changing electric field produces a 
magnetic field (Ampere's law with Maxwell's correction of equation 
\eqref{s2e16}). These two laws together ensure that the electric and magnetic
waves sustain each other and produce an electromagnetic wave.

\section{Electromagnetic waves}\label{s3}
Electromagnetic waves in free space are transverse. If we express the solutions
of equations \eqref{s2e17} and \eqref{s2e19} as
\begin{eqnarray}
\vb{E} &=& \vb{E}_0\exp(i(\vb{k}\cdot\vb{r} - \omega t)) \label{s3e1} \\
\vb{B} &=& \vb{B}_0\exp(i(\vb{k}\cdot\vb{r} - \omega t)) \label{s3e2} 
\end{eqnarray}
then equations \eqref{s2e13} and \eqref{s2e15} give
\begin{eqnarray}
\vb{k}\cdot\vb{E}_0 &=& 0 \label{s3e3} \\
\vb{k}\cdot\vb{B}_0 &=& 0. \label{s3e4}
\end{eqnarray}
from which we readily get $\vb{k}\cdot\vb{E} = 0$ and $\vb{k}\cdot\vb{B} = 0$.
Thus, the electric and magnetic fields are perpendicular to the direction of
propagation in free space. When electromagnetic waves are confined to wave
guides this is not the case \cite{griffiths2005introduction}. A wave traveling
through a hollow guide can have either the electric field transverse to the
direction of propagation or the magnetic field transverse to direction of
propagation, but not both. The former case is called `Transverse Electric' 
(TE) mode and the latter case is called `transverse magnetic' (TM) mode. A 
coaxial cable can support waves with both electric and magnetic fields 
transverse to the direction of propagation. These modes of propagation are
called `Transverse Electric (and) Magnetic' (TEM) modes.

\section{Electromagnetic waves in matter}\label{s4}
We can easily extend the treatment of section \ref{s2} to the case of 
electromagnetic waves in matter for which the linear relationships of
equations \eqref{s2e9} and \eqref{s2e10} hold good and which are devoid of 
free charges and currents. If we use these relations instead of those given 
by equations \eqref{s2e7} and \eqref{s2e8}, we get the wave equations of the 
form
\begin{eqnarray}
\laplacian\vb{E} &=& \mu\epsilon\pdv[2]{\vb{E}}{t} \label{s4e1} \\
\laplacian\vb{B} &=& \mu\epsilon\pdv[2]{\vb{B}}{t}. \label{s4e2}
\end{eqnarray}
The speed of electromagnetic waves is 
\begin{equation}\label{s4e3}
v = \frac{1}{\sqrt{\mu\epsilon}}.
\end{equation}
The ratio of the speed of electromagnetic waves in vacuum to that in materials
is the refractive index
\begin{equation}\label{s4e4}
n = \sqrt{\frac{\epsilon}{\epsilon_0}}\sqrt{\frac{\mu}{\mu_0}}.
\end{equation}
The permittivity and permeability of materials depends on the frequency of
electromagnetic fields. As a result, the refractive index is not a constant
but a frequency dependent quantity. The dependence of $n$ on the frequency
$\omega$ gives rise to dispersion of electromagnetic waves in media.

Let us now extend our analysis of the propagation of electromagnetic waves
in media which have free charges and which permit free currents. The current
density in response to the electric field is
\begin{equation}\label{s4e5}
\vb{J}_f = \sigma\vb{E}.
\end{equation}
When we use this relation in the equation of continuity of charges,
\begin{equation}\label{s4e6}
\pdv{\rho_f}{t} + \div\vb{J}_f = 0,
\end{equation}
we get
\begin{equation}\label{s4e7}
\pdv{\rho_f}{t} + \frac{\sigma}{\epsilon}\rho_f = 0,
\end{equation}
whose solution is
\begin{equation}\label{s4e8}
\rho_f(t) = \rho_f(0)\exp\left(-\frac{\sigma}{\epsilon}t\right).
\end{equation}
Thus, the density of free charges decays exponentially, the rate of decay being
faster for materials with higher conductivity. This behavior allows us to
consider $\rho_f(t)$ to be like a transient and ignore it from our analysis.
That is, we consider the steady state analysis when $\rho_f(t)$ is almost
zero. Maxwell equations in the steady state, after using equation \eqref{s4e5}
become
\begin{eqnarray}
\div\vb{E} &=& 0 \label{s4e9} \\
\curl\vb{E} &=& -\pdv{\vb{B}}{t} \label{s4e10} \\
\div\vb{B} &=& 0 \label{s4e11} \\
\curl\vb{B} &=& \mu\sigma\vb{E} + \mu\epsilon\pdv{\vb{E}}{t}. \label{s4e12}
\end{eqnarray}
Carrying out the same manipulations as we did in the case of vacuum, we get
the wave equations
\begin{eqnarray}
\laplacian\vb{E} &=& \mu\epsilon\pdv[2]{\vb{E}}{t} + \mu\sigma\pdv{\vb{E}}{t}
\label{s4e13} \\
\laplacian\vb{B} &=& \mu\epsilon\pdv[2]{\vb{B}}{t} + \mu\sigma\pdv{\vb{B}}{t}.
\label{s4e14} 
\end{eqnarray}
These are inhomogeneous wave equations. They admit plane wave solutions of
the form
\begin{eqnarray}
\vb{E}(\vb{r}, t) &=& \vb{E}_0\exp(i(\vb{k}\cdot\vb{r} - \omega t))
\label{s4e15} \\
\vb{B}(\vb{r}, t) &=& \vb{B}_0\exp(i(\vb{k}\cdot\vb{r} - \omega t)),
\label{s4e16} 
\end{eqnarray}
where it is understood that only the real part of the quantities have a 
physical significance. The wave number $k$ is also a complex quantity
\begin{equation}\label{s4e17}
k^2 = \mu\epsilon\omega^2 + i\mu\sigma\omega.
\end{equation}
One can write
\begin{equation}\label{s4e18}
k = k_+ + ik_-,
\end{equation}
where
\begin{equation}\label{s4e19}
k_\pm = \omega\frac{\epsilon\mu}{2}
\left(\sqrt{1 + \frac{\sigma^2}{\epsilon^2\omega^2}} \pm 1\right)^{1/2}
\end{equation}
The imaginary part $k_-$ of the wave number leads to an attenuation of the
electromagnetic waves in conducting media. The extent of attenuation is 
measured by the skin depth
\begin{equation}\label{s4e20}
d = \frac{1}{k_-},
\end{equation}
It is the depth in the material at which the amplitude of an incident
electromagnetic wave decays by a factor of $1/e$.

\section{Finite Difference Time Domain Method}\label{s5}
Finite difference methods are a way to solve differential equations numerically
by writing them as difference equations. The derivative of a function $f$
with respect to $x$ at a point $a$ can be approximated by
\begin{equation}\label{s5e1}
f^\opr(a) \approx \frac{f(a+h) - f(a)}{h}
\end{equation}
when $h$ is sufficiently small. The quantity $f(a+h) - f(a)$ is often called
the forward difference. A related quantity $f(a) - f(a - h)$ is called the
backward difference. The central difference is defined as
\[
f\left(a + \frac{h}{2}\right) - f\left(a - \frac{h}{2}\right).
\]
It is often convenient to approximate the derivative in equation \eqref{s5e1}
in terms of the central difference as
\begin{equation}\label{s5e2}
f^\opr(a) \approx \frac{1}{h}
\left(f\left(a + \frac{h}{2}\right) - f\left(a - \frac{h}{2}\right)\right).
\end{equation}
Once a differential equation is approximated by a difference equation, we can 
cast it as an equation involving matrices. There are excellent numerical
libraries to solve matrix equations which can be used to get an approximate
numerical solution of the differential equation.

The finite-difference time-domain method (FDTD) was developed by Kane Yee
\cite{yee1966numerical} so solve Maxwell equations numerically. The domain
in which the solution has to be found is divided into a grid of points at
which the derivatives of $\vb{E}$ and $\vb{B}$ are evaluated using the
central difference approximation of \eqref{s5e2}. The grid spans both space
and time variables. The `time-domain' refers to the fact that the fields are
analyzed as functions of time as opposed to taking their Fourier transforms
and writing them as functions of frequency. Since the method involves
functions of time and their derivatives with respect to time (and space)
it works for a wide range of frequencies. In fact, the frequency is one of
the parameters of an FDTD algorithm.

We will illustrate FDTD for a simple one-dimensional problem. We observed
in section \ref{s2} that an electromagnetic wave is sustained because a 
changing electric field produces a magnetic field and vice versa. For sake
of simplicity, let us consider the progagation of electromagnetic waves in
vacuum. The pair of equations from which the wave equation emerges is
\begin{eqnarray}
\curl\vb{H} &=& \epsilon_0\pdv{\vb{E}}{t} \label{s5e3} \\
\curl\vb{E} &=& -\mu_0\pdv{\vb{H}}{t} \label{s5e4}
\end{eqnarray}
Let us assume that the wave propagates in the $z$-direction with the 
electric field in the $x$-direction and the $\vb{H}$ field in the
$y$-direction. Equations \eqref{s5e3} and \eqref{s5e4} simplify to
\begin{eqnarray}
\pdv{E_x}{t} &=& -\frac{1}{\epsilon_0}\pdv{H_y}{z} \label{s5e5} \\
\pdv{H_y}{t} &=& -\frac{1}{\mu_0}\pdv{E_x}{z} \label{s5e6}
\end{eqnarray}
The fields $E_x$ and $H_y$ are functions of $z$ and $t$. In this case, the
domain of the problem is a segment along the $z$-axis. We divide it in by a 
set of grid points and evaluate the fields at each one of them at successive
instants of time. The numerical solution of the problem thus consists in
evaluating $E_x(z, t)$ and $H_y(z, t)$ at a discrete set of points. We can
approximate the derivatives in equations \eqref{s5e5} and \eqref{s5e6} in
terms of central differences and write \cite{sullivan2013electromagnetic}
\begin{eqnarray}
\frac{E_x(k, n + 1/2) - E_x(k, n - 1/2)}{\Delta t} 
 &=& -\frac{H_y(k + 1/2, n) - H_y(k - 1/2, n)}{\epsilon_0\Delta x}
\label{s5e7} \\
\frac{H_y(k + 1/2, n + 1) - H_y(k + 1/2, n)}{\Delta t} 
&=&
-\frac{E_x(k + 1, n + 1/2) - E_x(k - 1, n + 1/2)}{\epsilon_0\Delta x}
\nonumber \\
 & & \label{s5e8} 
\end{eqnarray}
We rewrite these equations as
\begin{eqnarray}
E_x\left(k ,n+\frac{1}{2}\right) &=& E_x\left(k ,
n-\frac{1}{2}\right) \nonumber \\
 & & -\frac{\Delta t}{\epsilon_0\Delta x}\left(\left(
H_y\left(k + \frac{1}{2}, n\right) - H_y\left(k - \frac{1}{2}, n\right)\right)
\right) \nonumber \\
 & & \label{s5e9}
\end{eqnarray}
and
\begin{eqnarray}
H_y\left(k+\frac{1}{2},n+1\right) &=& H_y\left(k+\frac{1}{2}, n\right) 
\nonumber \\
 & & -\frac{\Delta t}{\mu_0\Delta x}\left(
E_x\left(k + 1, n + \frac{1}{2}\right) - E_x\left(k - 1, n+\frac{1}{2}\right)
\right) \nonumber \\
 & & \label{s5e10}
\end{eqnarray}
A few observations about these equations:
\begin{itemize}
\item $E_x$ is computed at space coordinates $0, 1, \ldots$ and time 
coordinates $1/2, 3/2, \ldots$.
\item $H_y$ is computed at space coordinates $1/2, 3/2, \ldots$ and time 
coordinates $0, 1, \ldots$.
\item The points at which $E_x$ is computed are interleaved between the points
at which $H_y$ is computed in both space and time grid.
\end{itemize}
When these equations are implemented in code, we compute the fields at 
\emph{all} points in the domain \emph{at a given instant of time}. We repeat
this process for the desired number of time steps. 

There are a few more considerations when we implement these equations in code.
Equations \eqref{s5e9} and \eqref{s5e10} are very similar except that the
coefficient of the second term on the right hand side of the two equations
have $\epsilon_0$ in the first case and $\mu_0$ in the second case. The
values of these universal constants are $\epsilon_0 = 8.85418782\times 10^{-12}
$ m$^{-3}$kg$^{-1}$s$^4$A$^2$ and $\mu_0=1.25663706 \times 10^{-6}$ m kg
s$^{-2}$ A$^{-2}$. The two constants differ by $6$ orders of magnitude. In
order to bridge this gap, we introduce a new variable
\begin{equation}\label{s5e11}
\tilde{E}_x = \sqrt{\frac{\epsilon_0}{\mu_0}}E_x.
\end{equation}
In terms of this variable, equations \eqref{s5e9} and \eqref{s5e10} become
\begin{eqnarray}
\tilde{E}_x\left(k ,n+\frac{1}{2}\right) &=& \tilde{E}_x\left(k ,
n-\frac{1}{2}\right) \nonumber \\
 & & -\frac{\Delta t}{\mu_0\epsilon_0\Delta x}\left(\left(
H_y\left(k + \frac{1}{2}, n\right) - H_y\left(k - \frac{1}{2}, n\right)\right)
\right) \nonumber \\
 & & \label{s5e12}
\end{eqnarray}
and
\begin{eqnarray}
H_y\left(k+\frac{1}{2},n+1\right) &=& H_y\left(k+\frac{1}{2}, n\right) 
\nonumber \\
 & & -\frac{\Delta t}{\mu_0\epsilon_0\Delta x}\left(
\tilde{E}_x\left(k + 1, n + \frac{1}{2}\right) - \tilde{E}_x\left(k - 1, n+\frac{1}{2}\right)
\right) \nonumber \\
 & & \label{s5e13}
\end{eqnarray}
Recall from equation \eqref{s2e18} that $c = 1/\sqrt{\epsilon_0\mu_0}$ so that
we can write the two equations above as
\begin{equation}\label{s5e14}
\tilde{E}_x = \sqrt{\frac{\epsilon_0}{\mu_0}}E_x.
\end{equation}
In terms of this variable, equations \eqref{s5e9} and \eqref{s5e10} become
\begin{eqnarray}
\tilde{E}_x\left(k ,n+\frac{1}{2}\right) &=& \tilde{E}_x\left(k ,
n-\frac{1}{2}\right) \nonumber \\
 & & -\frac{c \Delta t}{\Delta x}\left(\left(
H_y\left(k + \frac{1}{2}, n\right) - H_y\left(k - \frac{1}{2}, n\right)\right)
\right) \nonumber \\
 & & \label{s5e15}
\end{eqnarray}
and
\begin{eqnarray}
H_y\left(k+\frac{1}{2},n+1\right) &=& H_y\left(k+\frac{1}{2}, n\right) 
\nonumber \\
 & & -\frac{c \Delta t}{\Delta x}\left(
\tilde{E}_x\left(k + 1, n + \frac{1}{2}\right) - \tilde{E}_x\left(k - 1, n+\frac{1}{2}\right)
\right) \nonumber \\
 & & \label{s5e16}
\end{eqnarray}
While implementing these equations in software we will have to choose certain
values for $\Delta x$ and $\Delta t$. $\Delta x$ is chosen in such a way that
there are at least ten points per wave. That is, if we are simulating a wave
with wavelength $\lambda$ then \cite{sullivan2013electromagnetic}
\begin{equation}\label{s5e17}
\Delta x \approx \frac{\lambda}{10}.
\end{equation}
The time step $\Delta t$ is determined by the Courant-Friedrichs-Lewy condition
\cite{cfl}. When we are solving a hyperbolic partial differential equation
numerically on a grid, we should choose a time step in such a way that it is
smaller than the time taken for the wave to travel to a neighboring point
a distance $\Delta x$ away. Electromagnetic waves need a time $\Delta x/c$
to travel from one grid point to the neighboring grid point. Therefore, we
choose 
\begin{equation}\label{s5e18}
\Delta t = \frac{\Delta x}{2c}.
\end{equation}
Using this equation in \eqref{s5e15} and \eqref{s5e16} we get
\begin{eqnarray}
\tilde{E}_x\left(k ,n+\frac{1}{2}\right) &=& \tilde{E}_x\left(k ,
n-\frac{1}{2}\right) \nonumber \\
 & & -\frac{1}{2}\left(\left(
H_y\left(k + \frac{1}{2}, n\right) - H_y\left(k - \frac{1}{2}, n\right)\right)
\right) \nonumber \\
 & & \label{s5e19}
\end{eqnarray}
and
\begin{eqnarray}
H_y\left(k+\frac{1}{2},n+1\right) &=& H_y\left(k+\frac{1}{2}, n\right) 
\nonumber \\
 & & -\frac{1}{2}\left(
\tilde{E}_x\left(k + 1, n + \frac{1}{2}\right) - \tilde{E}_x\left(k - 1, n+\frac{1}{2}\right)
\right) \nonumber \\
 & & \label{s5e20}
\end{eqnarray}
Equations \eqref{s5e19} and \eqref{s5e20} can be readily written in software
to simulate propagation of electromagnetic waves in one dimension.

So far we have considered the propagation of electromagnetic waves in 
vacuum. We now consider their propagation in dielectric and lossy media. In
the case of a dielectric medium, the first equation of the pair \eqref{s5e5}
and \eqref{s5e6} changes to
\begin{equation}\label{s5e21}
\pdv{E_x}{t} = -\frac{1}{\epsilon}\pdv{H_y}{z}.
\end{equation}
For linear dielectric, one can write
\begin{equation}\label{s5e22}
\epsilon = \kappa\epsilon_0,
\end{equation}
where $\kappa > 1$ is the relative permittivity, also called the dielectric
constant. If we follow the development of equations \eqref{s5e19} and
\eqref{s5e20} from Maxwell equation with the changes indicated in equations
\eqref{s5e21} and \eqref{s5e22}, we get
\begin{eqnarray}
\tilde{E}_x\left(k ,n+\frac{1}{2}\right) &=& \tilde{E}_x\left(k ,
n-\frac{1}{2\kappa}\right) \nonumber \\
 & & -\frac{1}{2\kappa}\left(\left(
H_y\left(k + \frac{1}{2}, n\right) - H_y\left(k - \frac{1}{2}, n\right)\right)
\right) \nonumber \\
 & & \label{s5e23}
\end{eqnarray}
and
\begin{eqnarray}
H_y\left(k+\frac{1}{2},n+1\right) &=& H_y\left(k+\frac{1}{2}, n\right) 
\nonumber \\
 & & -\frac{1}{2}\left(
\tilde{E}_x\left(k + 1, n + \frac{1}{2}\right) - \tilde{E}_x\left(k - 1, n+\frac{1}{2}\right)
\right) \nonumber \\
 & & \label{s5e24}
\end{eqnarray}
Equation \eqref{s5e24} is identical with \eqref{s5e20} while equation
\eqref{s5e23} differs from \eqref{s5e21} by the presence of dielectric 
constant in the coefficient of the second term on the right hand side.

In the case of electromagnetic waves propagating in a medium of dielectric
constant $\kappa$ and electric conductivity $\sigma$, Ampere's law becomes
\begin{equation}\label{s5e25}
\curl\vb{H} = \kappa\epsilon_0\pdv{\vb{E}}{t} - \sigma\vb{E},
\end{equation}
where we have used Ohm's law \eqref{s4e5}. The one dimensional equations
for the $H_y$ and $\tilde{E}_x$ (refer to \eqref{s5e11}) fields are
\begin{eqnarray}
\pdv{\tilde{E}_x}{t} &=& -\frac{c}{\kappa}\pdv{H_y}{z} - 
\frac{\sigma}{\kappa\epsilon_0}\tilde{E}_x \label{s5e26} \\
\pdv{\tilde{H}_y}{t} &=& -c\pdv{\tilde{E}_x}{z}. \label{s5e27}
\end{eqnarray}
If we follow the derivation of the difference equation from the differential
equations and write 
\[
\tilde{E}_x(k, n) = \frac{\tilde{E}_x(k, n + 1/2) + \tilde{E}_x(k, n - 1/2)}{2}
\]
then we get a pair of equation of which the second one is the same as 
\eqref{s5e24} but the first one is
\begin{eqnarray}
\tilde{E}_x\left(k ,n+\frac{1}{2}\right) &=& \frac{1-\alpha}{1+\alpha}
\tilde{E}_x\left(k , n-\frac{1}{2\kappa}\right) \nonumber \\
 & & -\frac{1}{2\kappa(1 + \alpha)}\left(\left(
H_y\left(k + \frac{1}{2}, n\right) - H_y\left(k - \frac{1}{2}, n\right)\right)
\right), \nonumber \\
 & & \label{s5e28}
\end{eqnarray}
where \cite{sullivan2013electromagnetic}
\begin{equation}\label{s5e29}
\alpha = \frac{\sigma\Delta t}{2\kappa\epsilon_0}.
\end{equation}

\section{Some practical matters}\label{s6}
\subsection{Absorbing boundary condition}
\subsection{Perfectly matching layer}

\section{A simple example of FDTD}\label{s7}


\bibliographystyle{plain}
\bibliography{em}
\end{document}

