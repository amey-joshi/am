\documentclass{article}
\usepackage{amsmath, amssymb, amsthm, amsfonts}
\usepackage{url}
\numberwithin{equation}{section}
\DeclareMathOperator{\pr}{P}
\DeclareMathOperator{\E}{E}
\linespread{1.3}
\title{A Review of Mortality Models}
\author{To be filled}
\date{Sep-2020}
\begin{document}
\maketitle
\section{Introduction}\label{s0}
Although it is very difficult to predict the behavior of a member of a large
population one can often make statistical statements about the same population 
with much greater 
confidence. For example, it is impossible to predict when a particular 
radioactive atom will decay but one can make an accurate prediction of how 
many atoms will decay in a long enough time interval. Likewise, it is not
possible to predict the time of death of an individual although one can reliably
predict how many people from a large enough group will survive past a certain
date in future. Actuaries knew this statistical fact long before the 
mathematical foundations of the theory of probability were laid. The 
mathematical models they to describe how deaths occur in a given society are
called `mortality models'.

The death of an individual in a population or the decay of a radioactive atom
is an event happening to a member of the population. Likewise, the failure of
a machine, an occurrence of an event, the duration of a certain phase in an
economy are all different manifestations of the same phenomenon in which 
something happens that significantly alters the future of the member of the
population. Death terminates an individual's existence, radioactive decay
transforms the atom, failure renders a machine unusable and economies change 
after occurrences of certain cataclysmic events. Mortality of an individual
is akin to reliability of a machine. Life of an individual is similar to
the duration of a certain phase of business cycle. As a result, the 
theoretical tools used to study these outwardly different phenomena are similar 
to each other. 
Mortality analysis in actuarial science is called survival analysis is 
statistics, reliability theory in engineering, duration analysis in economics 
and event history analysis in sociology.

Traditionally, mortality models studied existing demographic data to produce
mortality tables. They described mortality at a fixed point in time. The pace 
of technology development was quite slow in the times when the first mortality
models were proposed. Therefore, the mortality data and the models describing
them appeared to be like a natural characteristic of the human species. They
sufficed the needs of the fledgling insurance industry to compute projections
of annuities.

However, the advancement in health science since the early years of the 
twentieth century, the relative peace in the western world since the second 
world war and abundance of food in the rich world resulted in a change in
historical mortality patterns. Pension funds started realizing that annuities
were being drawn for longer than expected periods. As the population in the
developed countries started graying the pension funds had the additional 
problem of a higher annuity obligations and a lower contribution from newer
workers. Pension funds now have a keen interest in understanding how mortality 
rates will evolve in the future. They now want to forecast mortality than just
produce models of the current mortality. 

Demographic forecasting predates mortality forecasting. Economists like Thomas
Malthus predicted that the human population will grow to such an extent that
there will not be enough to feed everyone. This dire prediction meant that the
politicians and the economists had a keen desire to predict how populations
will grow and whether there will be enough food for everyone. As a result, many 
statistical methods developed in Demography were readily available for mortality
forecasting. This need for forecasting was also coeval with the developments
in the modern theory of mathematical statistics and the foundations of
probability. It is not a coincidence that the early mortality models were
mathematical functions while the later ones are stochastic models.

Mortality models, being mathematical functions, produce a single curve for
a given set of its parameters. On the other hand, mortality forecasts are a
collection of curves or a family of probability distributions. Their output
is accompanied with appropriate confidence intervals to indicate the 
uncertainty in their reported values.

Before we start reviewing the deterministic and stochastic mortality models
let us describe certain characteristic features of human mortality. Although
the life expectancy at birth has significantly improved in the post second
world war years the maximum life span seems to be pretty constant throughout 
history. A new born child faces a great risk of dying as it adjusts itself
to its new environment, warding off pathogens and fighting diseases. As time 
goes by the child starts developing immunity against many germs and its
chances of dying begin to drop. This drop continues until early adulthood 
when there is a sudden jump in mortality. It happens because of aggressive
behavior in the late teens in both sexes and because of maternal mortality
in females. The spike in mortality eventually dwindles into a monotonic
increase in mortality as senescence catches up. We will see that there is 
a rich collection of mathematical models describing these characteristics
of human mortality. The basic structure of the mortality behavior also
suggests that it might be profitable to fit a different model for different
stages of human life. The earliest models indeed did not cover childhood or
take into account the spike in mortality starting late teens. They were 
simple mathematical functions that fit a portion of the mortality curve that
was of most relevance to the insurance companies. Later models continued to
be deterministic in nature but had more number of parameters that could be
fitted for a given empirical data. However, it was not always possible to 
ascribe an unambiguous biological meaning to all parameters. Further, their
complexity was also controlled by the limitations of the computing technology
of their period.

We will sketch the historical development of mortality model, both 
deterministic and stochastic, starting from de Moivre's model in the next
section. We also define basic terms commonly used in actuarial science and 
demonstrate how to evaluate their expressions in simple cases. In section 3
we describe the Gompertz's model, the first realistic mortality model that 
was used for a very long time in the industry. The success of Gompertz model
lay in the number of successors it produced. Of these, we cover Makekam's 
model and its generalizations in section 4. In section 5 we mention a few
other mortality models used in European countries but whose significance today
lies mostly in their historical value. The $8$-parameter Heligman-Pollard model
was the first one that could be fitted to the entire mortality curve, starting
from infancy to very advanced years. We describe it in section 6. Starting
section 7 we turn our attention to stochastic models, starting with the
celebrated Lee-Carter model. An interesting class of models used across social
sciences is the Age-Period-Cohort models. We review a few of them relevant
to mortality in section 8. In section 9 we consider the Cairns-Blake-Dowd model
and finally we consider stochastic models with cohort effects in section 10. 
The models considered so far have looked at mortality irrespective of the
cause of death. However, in recent years, death rate due to certain causes
is falling while those due to others is rising. One can, therefore, forecast
death rates by cause and aggregate the results. This is the approach taken by
the cause-of-death models in section \ref{s11}. Several mortality models can
be written in the form $f(\mu) = L(\theta)$ where $L$ is a linear function of
the parameters $\theta$ of the model and $f$ is some function of the force
of mortality that permits us to have the right hand side as $L(\theta)$. The
statistically rigorous method to fit such models to mortality data is called
the generalized linear models. Section \ref{s12} discusses them. Not all
models can be brought in this format. Important examples being Lee-Carter and
Renshaw-Haberman. One has to take recourse to spline regression in these
cases. They are the topic of section \ref{s13}.

\section{de Moivre's law}\label{s1}
An annuity is a lifelong payment made to an individual by a company. The 
company would want to estimate how much annuity its customers will consume
before they cease to exist. In order to estimate its yearly obligations, the
company would need to know what proportion of its customers will survive
past various points in time in the future. It to this problem that de Moivre 
\cite{de1731annuities} addressed his treatise. In order to estimate the value
of annuities one must consider the interest earned by the company's assets and
the probability of survival of its customers. In de Moivre's days the interest
rates were controlled by law. Therefore, he focused on estimating the 
probability of survival of the company's customers.

Developed in the 18th century, de Moivre's model was indeed quite simple. 
He assumed that there is a certain age, $\omega$ beyond which no human 
survives. Further, as time goes by the probability of a person's survival
decreases. The probability that a new born person will survive at least
$x$ years was proposed to be
\begin{equation}\label{s1e1}
s(x) = 1 - \frac{x}{\omega}, 
\end{equation}
where $0 \le x \le \omega$. The function $s$, called the survival function,
has the following characteristics
\begin{enumerate}
\item It is a decreasing function of $x$.
\item $s(x = 0) = 1$ and $s(x = \omega) = 0$. In between these age limits, it
is a linear function of $x$.
\end{enumerate}
de Moivre proposed the use of different $\omega$'s for different age groups, 
making his model piece-wise linear. We will examine a few immediate 
consequences of de Moivre's model and while doing so we will introduce the
notation common in actuarial literature \cite{jordan1967society}. The symbol
$(x)$ denotes life aged $x$. If $k$ is an estimate of the size of a population
born together then the number of people alive at age $x$ is $l_x = ks(x)$.
The number of people who will die in the interval $x$ to $x + 1$ is $d_x = l_x
- l_{x+1}$. The probability that $x$ will survive for another $n$ years is
\begin{equation}\label{s1e2}
{}_np_x = \frac{l_{x+n}}{l_x} = \frac{s(x+n)}{s(x)}.
\end{equation}
The probability that a person will die within next $n$ years of $x$ is
\begin{equation}\label{s1e3}
{}_nq_x = 1 - {}_np_x.
\end{equation}
It is common to denote ${}_1p_x$ as $p_x$ and ${}_nq_x$ as $q_x$. The `force
of mortality' is defined as
\begin{equation}\label{s1e4}
\mu(x) = -\frac{s^\prime(x)}{s(x)}, 
\end{equation}
where a $s^\prime$ denotes the derivative of $s$ with respect to $x$. The 
force of mortality is also called the hazard function in the statistical
literature. The hazard function or the `force of mortality' is an individual's
susceptibility to die. Its reciprocal is the individual's resistance to death. 
de Moivre's law proposes that a person's resistance to die varies linearly over
time. That is,
\begin{equation}\label{s1e5}
\nu(x) = \frac{1}{\mu(x)} = \omega - x.
\end{equation}
If $x_0 = a, x_1 = a + d, x_2 = a + 2d, ..., x_n = a + nd$ then it is clear 
that $\nu(x_n)$ form an algebraic progression. This point may seem to be just a 
mathematical characteristic at this stage. We shall see in the next section that
it marked a departure from de Moivre's model to a more realistic one. For de 
Moivre's law of mortality, it is easy to confirm that
\begin{eqnarray}
{}_np_x &=& \frac{\omega - (x + n)}{\omega - x} \label{s1e6} \\
{}_nq_x &=& \frac{n}{\omega - x} \label{s1e7} \\
\mu(x) &=& \frac{1}{\omega - x} \label{s1e8}
\end{eqnarray}
de Moivre himself did not consider his law to be an accurate description of
human mortality. He emphasized in his treatise that it was an approximation
useful to calculate the annuities. His piece-wise linear approximation was a
valuable device to carry out practical computations in the eighteenth century.

\section{Gompertz's law}\label{s2}
An immediate consequence of de Moivre's law is that an individual's resistance
to death diminishes algebraically as time goes by. The English actuary 
Benjamin Gompertz observed \cite{gompertz1825xxiv} that the resistance to 
death decreases geometrically instead. If one considers small enough time 
intervals then at the end of every interval a person's resistance to death 
reduces by the same proportion. Expressed mathematically, it means that
\begin{equation}\label{s2e1}
\frac{d\nu}{dx} = -k\nu(x),
\end{equation}
where the function $\nu$ was defined in equation \eqref{s1e5}. It immediately
follows that
\begin{equation}\label{s2e2}
\log\nu(x) = -kx + \log B,
\end{equation}
where $B$ is a constant of integration. The `force of mortality' or the
hazard function is therefore
\begin{equation}\label{s2e3}
\mu(x) = Be^{kx}.
\end{equation}
The constant $B$ is called baseline mortality and $k$ is called the 
senescent component.
An individual's susceptibility to die thus increases exponentially with age.
From equation \eqref{s1e4} and \eqref{s2e3}, the differential equation for
the survival function is
\begin{equation}\label{s2e4}
\frac{s^\prime(x)}{s(x)} = Be^{kx}.
\end{equation}
The solution of this ordinary differential equation is
\begin{equation}\label{s2e5}
s(x) = C\exp\left(\frac{B}{k}\exp(kx)\right),
\end{equation}
where $C$ is another constant of integration. The constants $B$ and $C$ in
equation \eqref{s2e5} are determined from the mortality data of the population
under study. It is common to introduce a constant $c = e^k$ and write equation
\eqref{s2e5} as
\begin{equation}\label{s2e6}
s(x) = C\exp\left(\frac{B}{k}c^x\right),
\end{equation}
Gompertz suggested that his formula is applicable for the age
group between $10-15$ years of age up to $55-60$. In order to extend it beyond
the age of $60$, Gompertz recommended that a different set of constants $B$
and $C$ be used.

Gompertz was aware that death in a population has at least two factors. One
of them is the deterioration in the individual's resistance to death. The 
other one is chance alone. The first factor depends on the individual's age
while the second factor is constant throughout. However, in the formulation
of his law of mortality, Gompertz considered only the first factor. Gompertz
could have fitted simpler polynomial functions to the mortality data available
in his times. However, instead of trying to fit a function to the data he
developed his model based on an observation root in biology that organisms
deteriorate with age in a geometric proportion. Gompertz model was not only a
better fit to data than de Moivre's but it also started the tradition of
finding a biological explanation for every term in a mortality model. It is
important to note that Gompertz considered death as just a biological 
phenomenon. His law is best applicable to populations well protected from 
wars or pestilence. It is, therefore, an upper limit to longevity in a
population. In that sense, it served to be a useful estimate of the insurance
companies' liabilities. The success of Gompertz's law in practical calculations
ensured that for several decades newer mortality laws were only an improvement
over it. In the next section, we shall consider a few of them.

\section{Makeham's law and its generalizations}\label{s3}
William Makeham, the English mathematician and actuary, added a term to
Gompertz's law of resistance to death which the latter had acknowledged but
not included. Makeham's modification \cite{makeham1867law} of equation 
\eqref{s2e3} is
\begin{equation}\label{s3e1}
\mu(x) = A + Be^{kx},
\end{equation}
where $A$ is a constant independent of the person's age. Makeham's survival
function is
\begin{equation}\label{s3e2}
s(x) = C\exp\left(Ax + \frac{B}{k}\exp(kx)\right).
\end{equation}
Makeham's survival function has three constants $A, B$ and $C$ and therefore
can be fitted to empirically observed mortality data more effectively than
Gompertz's model. If we write $c = e^k$ then we can write equation \eqref{s3e2}
in its customary form
\[
s(x) = C\exp\left(Ax + \frac{B}{k}c^x\right).
\]
Makeham's law was often applied to mortality tables in the
age range beyond $20$ years to almost end of life\cite{jordan1967society}. 
In 1890\cite{makeham1890further}, Makeham proposed yet another modification to 
his law by adding a linear term to the hazard function or the force of 
mortality. Makeham thought that in addition to the two factors spotted by
Gompertz there is another one that grows linearly with age. It was proposed to 
be
\begin{equation}\label{s3e3}
\mu(x) = A_1 + A_2x + Be^{kx}.
\end{equation}
The resulting survival function is
\begin{equation}\label{s3e4}
s(x) = C\exp\left(A_1x + \frac{A_2}{2}x^2 + \frac{B}{k}\exp(kx)\right).
\end{equation}
Makeham's law, like its predecessor, Gompertz's law was used to construct
mortality tables of adults. Their simplicity allowed them to be used 
effectively up to as late as the 1940s. Gompertz law was used to construct
the 1937 Standard Annuity Table and Makeham's law for 1940 Annuity table
\cite{jordan1967society}. Both laws assumed that the force of mortality 
increases monotonically with time. However, it is commonly observed that 
infant mortality contradicts this assumption. This is the reason why Gompertz's
and Makeham's laws were applied only to populations over $20$ years of age. 

Mortality decreases from the birth of the child until the onset of adulthood. 
In order to extend Makeham's law of 1860 to infantile ages, Wilhelm Lazarus 
\cite{lazarus1867ueber} added an exponentially decreasing term. His law 
expressed the force of mortality as 
\begin{equation}\label{s3e5}
\mu(x) = B_1 e^{-k_1x} + A + B_2e^{k_2 x}.
\end{equation}
The constants $B_1$ and $B_2$ are positive numbers. The corresponding survival 
function is
\begin{equation}\label{s3e6}
s(x) = 
C\exp\left(-\frac{B_1}{k_1}e^{-k_1x} + Ax + \frac{B_2}{k_2}e^{k_2x}\right)
\end{equation}

The way infants' susceptibility to die of diseases upsets Gompertz's and 
Makeham's assumption of monotonicity of the law of mortality, so does the 
spike in the death rate of young adults. Wars were frequent in Europe of the 
eighteenth century and the victims were disproportionately the young recruits.
Even otherwise, young adults are more prone to death by accidents. Maternal
mortality too was quite high in the eighteenth century. In order
to take into account this mortality hump in the young, Thiele 
\cite{thiele1871mathematical} introduced a Gaussian term to Lazarus' model.
His law of mortality is
\begin{equation}\label{s3e7}
\mu(x) = B_1 e^{-k_1x} + Ae^{-a_1(x - a_2)^2} + B_2e^{k_2 x}.
\end{equation}
All the constants in this equation, namely $A, a_1, a_2, B_1, k_1, B_2$ and 
$k_2$ are positive numbers. The presence of the Gaussian term prevents us
from writing a closed form expression of the survival function. However, the
ordinary differential equation for the survival function can be readily 
integrated numerically. Alternatively, if the initial conditions are available,
one can write its solution in terms of the error function.

We observe that all generalizations of Makeham's laws introduce additional
constants to the force of mortality and the hazard function. These constants
provide additional `degrees of freedom' to the functions used to fit the 
mortality data, resulting in a better fit.

Forfar, McCutcheon and Wilkie \cite{forfar1988graduation} proposed a far wider 
generalization of the Gompertz and Makeham's laws. They defined a class of 
models, called GM models after their inventors, with the mathematical form
\begin{equation}\label{s3e8}
\mu(x) = \sum_{i=0}^{r-1}\alpha_i x^i + 
\exp\left(\sum_{i=1}^{s-1}\beta_i x^i\right).
\end{equation}
They also used the convention that if $r = 0$ then the polynomial terms are 
absent and if $s = 0$ then the exponential terms do not appear. They called
the law of force of mortality of equation \eqref{s3e8} as $\mathrm{GM}(r,s)$.
It is easy to check that Gompertz's law is $\mathrm{GM}(0, 2)$, Makeham's
first law is $\mathrm{GM}(1,2)$ and Makeham's second law is $\mathrm{GM}(2,2)$.
The $\mathrm{GM}(0, 2)$, $\mathrm{GM}(2,2)$ and $\mathrm{GM}(1,3)$ are used
by the Continuous Mortality Investigation Bureau\cite{pitacco2016high} in the 
United Kingdom.

Gompertz's model and its descendants considered in this section had simple
algebraic forms, each term of which could be interpreted in biological
terms. However, they were not always easy to fit on available mortality
data. The presence of exponentials also made them numerically unstable. 
Moreover, they covered only a limited number of sources of mortality.

\section{Other deterministic mortality laws}\label{s4}
de Moivre's, Gompertz's and Makeham's laws of mortality were not the only
ones in vogue in the nineteenth century and prior to it. In this section we
mention a few notable ones without commenting on their merits. Lambert's
survival function \cite{lambert1776dottrina} was
\begin{equation}\label{s4e1}
s(x) = \left(\frac{a - x}{x}\right)^2 - b\left(e^{-x/c} - e^{-x/d}\right).
\end{equation}
It had four degrees of freedom. Babbage\cite{babbage1823tables} proposed a 
quadratic survival function
\begin{equation}\label{s4e2}
s(x) = c - bx - ax^2.
\end{equation}
Young proposed a polynomial of degree $40$ as the survival function in 
\cite{adler1866memoir} 1826. von Littrow\cite{von1832lebensversicherungen}
extended Babbage's model to polynomials of higher degree. Moser
\cite{moser1839gesetze} proposed a polynomial survival function with 
fractional powers of $x$. It has five degrees of freedom and had the form
\begin{equation}\label{s4e3}
s(x) = 1 - ax^{1/4} + bx^{9/4} - cx^{17/4} - dx^{25/4} + ex^{33/4}.
\end{equation}
The original mortality laws of Gompertz and Makeham were applicable only
for adults. Oppermann proposed \cite{oppermann1870graduation} a law for people 
younger than $20$ years of age. It was
\begin{equation}\label{s4e4}
\mu(x) = \frac{a}{\sqrt{x}} + b + cx^{1/3},
\end{equation}
where $a, b, c$ are parameter selected to fit the data. Perks 
\cite{perks1932some} proposed two models of the form
\begin{eqnarray}
\mu_1(x) &=& \frac{A + Bc^x}{1 + Dc^x} \label{s4e5} \\
\mu_2(x) &=& \frac{A + Bc^x}{Kc^{-x} + 1 + Dc^x} \label{s4e6}
\end{eqnarray}
The denominators in these forms flatten the exponential increase in Gompertz
form, especially at advanced age. The mortality law of equation \eqref{s4e5}
was used to graduate table of annuitant lives in the United Kingdom. Beard
\cite{beard1971some} wrote his law in terms of the probability that a life 
aged $x$ will die in the next year. It was
\begin{equation}\label{s4e7}
q_x = A + \frac{Bc^x}{Ex^{-2x} + 1 + Dc^x},
\end{equation}
where $A, B, c, D, E$ are parameters selected to fit the mortality data.
Barnett's formula appears on page 141 of \cite{joint1974considerations}. It
fits data using four parameters and has the mathematical form
\begin{equation}\label{s4e8}
q_x = \frac{A - Hx - Bc^x}{1 + A - Hx + Bc^x}.
\end{equation}
Wilkie \cite{wilkie1976international} generalized this form to
\begin{equation}\label{s4e9}
q_x = \frac{f(x)}{1 + f(x)},
\end{equation}
where
\begin{equation}\label{s4e10}
f(x) = \exp\left(\sum_{i=1}^s\alpha_i x^{i-1}\right).
\end{equation}
All these models were attempts to fit the mortality data with a continuous 
function. Many were increasingly complicated functions that were selected on
the sole merit that they fitted the observed data well. They were not rooted in 
demographic observations like the models of Gompertz, Makeham, Lazarus or 
Thiele. As a result, they soon fell into disuse after stochastic models came on
the scene.

\section{Heligman-Pollard model}\label{s5}
The term `odds' is a well-defined term in probability theory. When we are 
dealing with an experiment with two outcomes - that something happens or 
it does not - then it is common to express the probabilities in terms of odds. 
For example, if $p$ is the probability that an event will happen and $q = 1 - p$
is the probability that it will not happen then the odds of the experiment is 
$p/q$. The usage of odds instead of probabilities is very common in gambling or
betting.

Recall from equations \eqref{s1e2} and \eqref{s1e3} that $p_x$ is the 
probability that life aged $x$ will survive for another year and $q_x$ is
the probability of the complimentary event. Therefore, the odds of death in 
this case are $q_x/p_x$.  Heligman and Pollard \cite{heligman1980age} proposed 
a model that has the algebraic structure of Thiele's mortality law but 
applied to odds of survival. Mathematically it is expressed as
\begin{equation}\label{s5e1}
\frac{q_x}{p_x} = A^{(x+B)^C}+
D\exp\left(-E\log^2\left(\frac{x}{F}\right)\right) + GH^x.
\end{equation}
The model has eight parameters, $A, B, C, D, E, F, G$ and $H$. Of these, $A,
B, C, D \in (0, 1)$, $E > 0$, $F \in (15, \omega)$, $G \in (0, 1)$ and $H
> 0$. The constant $\omega$ is the hump maximum and it can be between $55$
and $100$. The Heligman-Pollard model was first developed for the Australian
population post the second world war. An advantage of Heligman-Pollard model 
is in biological interpretation of each of the three terms on the right hand 
side of equation \eqref{s5e1}.

The first term on the right hand side of equation \eqref{s5e1} represent 
infantile mortality. It is a fast declining exponential representing the 
rapid fall in mortality during infancy. $A$ is approximately equal to $q_1$ 
and is a measure of the mortality rate of a one year old child. $C$ is a 
measure of how quickly the new-born adjusts to its surroundings and gains 
immunity. A higher $C$ indicates the fast decline in infant mortality as the
child passes through infancy. When $B = 0$, $q_0 = 0.5$ irrespective of the
values taken by $A$ and $C$. The value of $q_0$ is closer to $q_1$ if $B$ is
higher for a given value of $C$. $B$ is thus an indicator of the central 
tendency of infant mortality. 

The second term is Gaussian in the logarithm of age. It represent the accident
mortality for young adults of both sexes and maternal mortality in young 
females. It covers the `accident hump` observed in all demographic data. It
is prominently observed between ages $10$ and $40$. The parameter $D$ controls 
the severity of accident hump, $E$ determines the spread of the hump and $F$ 
the location.  Using a statistical analogy, $E$ is a measure of spread and 
$F$ that of the central tendency. 

The third term traces its roots to the Gompertz model. It represents the
exponential increase in the deterioration of the body. The parameter $G$ 
represents the senescent mortality and $H$ the increase in the rate of that
process.

It is possible to write Heligman-Pollard model in the form
\begin{equation}\label{s5e2}
q_x = \frac{f(x)}{1 + f(x)},
\end{equation}
where
\begin{equation}\label{s5e3}
f(x) = A^{(x+B)^C} + D\exp\left(-E\left(\log\frac{x}{F}\right)^2\right) + 
GH^x.
\end{equation}
A variation of the function $f$ in to
\begin{equation}\label{s5e4}
f(x) = A^{(x+B)^C} + D\exp\left(-E\left(\log\frac{x}{F}\right)^2\right) + 
\frac{GH^x}{1 + GH^x}
\end{equation}
leads to the second form of Heligman-Pollard model. The two functions defined
in equations \eqref{s5e2} and \eqref{s5e3} are not too dissimilar for most
part of human mortality values. Two more variations of the Heligman-Pollard
model are available in literature\cite{heligman1980age}.

The advantages of Heligman-Pollard model \cite{umar2019modeling} lie in its
smoothness, interpolability, parsimony, analytic manipulability and in 
easy comparison with other models. Like all deterministic models, it models
mortality at a fixed point time for all ages. As a result it used to construct
age-period tables. Its shortcomings are because of the numerical instability
in the algorithms used to find the parameters. It also has a wide range of
parameters all producing a similar fit.


\section{Lee-Carter model}\label{s6}
The problem of forecasting population changes and composition is of interest
to both the demographers and the pension fund managers. Ronald Lee and 
Lawrence Carter's motivation for developing a model to forecast mortality was
the observation that the human life expectancy rose from $47$ years to $75$ 
years in the United States in the $88$ years starting from the year $1900$.
Should this trend continue unabated in the future, the human life expectancy
will touch $100$ years by the year $2065$ upsetting the Social Security 
Administration's calculations based on the long-term life expectancy of about
$80.5$ years. The wide difference between the predictions of the linear 
increase in life expectancy 
\cite{lee1992modeling} and the assumptions of the pension funds 
made the actuaries turn their attention to forecasting than modeling alone.

Unlike the previous models which tried to build formulae that mimicked the
behavior of human mortality in various stages of life, Lee and Carter used
the statistical techniques of time series for their projection into the future.
They excluded factors representing advancement in medicine or the behavioral
and social changes on mortality. Their time series model extrapolated the 
long-term trends seen in demographic data over several decades. They did not
find it necessary to assume that the human life cannot extend beyond a certain
limit. Nothing analogous to the $\omega$ in de Moivre's model would be found
in Lee and Carter's.

The mathematical form of Lee and Carter's model is
\begin{equation}\label{s6e1}
\log{M}(x, t) = {a}_x + {b}_x{k}_t + \epsilon_{x, t},
\end{equation}
where the coefficients ${a}_x$ and ${b}_x$ depend on the age, the
`index' ${k}_t$ depends on time and ${M}(x, t)$ is the mean death
rate for age $x$ in time $t$. Since
\[
\frac{d}{dt}{M}(x, t) = {b}_x\frac{dk_t}{dt},
\]
${b}_x$ captures the the changes in mortality rates due to changes in
the index with time. A positive value of a component of ${b}_x$ indicates
falling mortality while a negative value indicates the opposite. The term
$\epsilon_{x, t}$ is an `error term' with mean $0$ and variance that depends
on the age. It captures the factors that influence mortality but are not
explicitly included in the model.

The model of equation \eqref{s6e1} does not have a unique solution. If 
${a}$, ${b}$ and ${k}$ form one solution then for any number $c$,
the triple ${a} - c{b}, {b}, {k} + c$ is also a solution. In
fact, even ${a}, c{b}, {k}/c$ is a solution if $c \ne 0$. This phenomenon is
also called `identifiability problem'\cite{plat2009stochastic}.

Fitting the Lee-Carter model consists in finding the parameters ${a}_x,
{b}_x$ and ${k}_t$ for a given matrix of mortality rates ${M}(x, t)$.
The model is solved using the singular value decomposition (SVD). The algorithm
is described in appendix A to Lee and Carter's paper\cite{lee1992modeling} 
introducing the model.

Lee and Carter's forecast of life expectancy for the American population was 
$86.05$ by the year $2065$. The Actuary of the Social Security Administration,
on the other hand, forecast it as $80.5$. In $2020$ the observed value is 
$78.87$, suggesting that Lee and Carter's forecast might be closer to the 
reality in the year $2065$.

The advantages of Lee-Carter model lie in its robustness, simplicity and 
the fact that it fits well over wide ranges of age. It is also observed to be
able to fit small non-linearities in the mortality curve quite well. Its 
shortcomings are its lack of smoothness of age effects in small populations, 
inability to work with cohorts, inability to cope with improvements varying 
across times and a possible underestimation of uncertainty. It also has a 
limited versatility owing to it being a one-factor model.

\section{Age-Period-Cohort models}\label{s7}
Demographic data is often analyzed by taking into account the age, period 
and the cohort of individuals. The analysis need not be for mortality alone.
It could be for incidence of diseases or even economic consumption. Consumption
patterns do depend on age of the individual. But when they are observed over
a long enough duration they are also found to depend on the period and the 
cohort. For instance, certain goods may not have been available or would have
been expensive in a certain period and therefore we not consumed by many. It
is also well-known that different age groups of people have different tastes.
The age-period-cohort (APC) models are a class of additive models where the
predictor is a sum of three time effects, age, period and cohort. APC models
are thus able to take into account the impact of the changes in the society 
and the available technology.

Period analysis alone is quite useful to understanding the effect of wars,
pandemics, revolutions or other upheavals on a population. However, they
capture the effect of the times in which people are living. Behavioral
changes in people are better captured by considering cohorts. The 
characteristics of a cohort are an aggregate of the characteristics of its 
members\cite{willekens1991age}. Modern APC models require longitudinal data
to understand the life processes as they evolve. They do not consider just
the event of the death of an individual but the process of dying whose 
culmination is death. They take the view that the mortality risks in a 
population are better determined by an understanding of morbidity in it
\cite{van1990determinants}.

The APC framework is used widely in the social sciences. In the context of
mortality modeling and forecasting it started being used while answering the
questions \cite{willekens1991age}:
\begin{itemize}
\item Does mortality show a greater regularity when viewed at the level of a
cohort or for a particular period? The answer to this question is important
for mortality forecasting.

Although a full-fledged APC framework arose relatively recently, the importance
of cohort analysis was not missed by early demographers and actuaries. Derrick
\cite{derrick1927observations} observed in 1927 that the ratio of mortality
for one cohort to that of another was constant for all ages above $10$ years.
It suggested that it may be possible to forecast the mortality of an individual
based on the cohort in which he falls. It was also observed that the mortality
rates for the generation that was young during the years of the second world
war were overestimated for obvious reasons. This once again indicates the 
importance and the role of cohorts in modeling mortality. 

\item Do events early in life affect the experiences later in life? This 
question can be answered only in the context of a cohort analysis.

While studying the death rates in England, Scotland and Sweden, Kermack
\cite{kermack1934death} observed that the differences in mortality was mostly
because of the environmental conditions experienced in the first fifteen years
of life. It was not a result of the other influences in later life. The 
study also uncovered the fact that an improvement in the conditions around
child-birth boosted survival rates of new-borns. It proposed that the health
of the mother has a positive impact on the health of the child and its chances
to live longer. Preston and de Walle \cite{preston1978urban} came to similar
conclusion for the French mortality data. This idea was further supported by 
Caselli and Capocaccia's study\cite{caselli1989age} on Italian demographics 
that infant mortality had a weakening effect on eventual mortality. However, 
this observation is not universally accepted. Other studies indicate that a 
high infantile mortality results in lower eventual mortality because the 
`stronger' individuals stay long enough to live long enough 
\cite{manton1981methods}. Lest one concludes that only childhood environment
determines mortality rate later in life, Horiuchi reported 
\cite{horiuchi1983long} that the malnutrition and physical privation during the
second world war wreaked the health of those who lived through it and affected
their mortality when they became old.
\end{itemize}

APC studies are plagued by what is called the `identification problem' common
to all cohort analysis. It is the problem of assigning an individual to the
right cohort when the a) the cohorts are adjacent in time, b) the individual 
is on the border line of two adjacent cohorts and c) the characteristics of
the cohorts are quite distinct. The problem can be mitigated by following
the directions suggested in Willekens and Scherbov's \cite{willekens1991age}
analysis.

APC model can be expressed as a generalized linear model in which we predict
the mortality rate from age, period and cohort as predictors. We usually use
the variables in the data that are the most appropriate proxy for age, period
and cohort. Once the model is build from the available data, we can use it to 
forecast mortality for a newer set of predictors. It is assumed that the 
mortality rate can be described as a Poisson process. This assumption is found 
to be valid for societies with low death rates. It may not hold good when 
the society is experiencing a war or a pandemic.


\section{Cairns-Blake-Dowd model}\label{s8}
Cairns, Blake and Dowd proposed a model \cite{cairns2006two} to study the
evolution of mortality beyond sixty years of age and its impact on longevity
risk. It was a two factor stochastic model. Of these, the first factor affects
the mortality rate at all ages in an identical manner and the second one 
affects the advanced years to a greater extent. Their mortality curve is
defined in terms of forward survival probability $\pr(t, T_0, T_1, x)$. It is
the probability measured at $t$ that an individual aged $x$ at time $0$,
alive at $T_0$ will survive until time $T_1 > T_0$. The curve is represented
by the equation
\begin{equation}\label{s8e1}
1 - p(t+1,t,t+1,x) = 
\frac{e^{A_1(t+1)+A_2(t+1)(x+t)}}{1+e^{A_1(t+1)+A_2(t+1)(x+t)}},
\end{equation}
where $A_1(u)$ and $A_2(u)$ are stochastic processes. If $q(p, T_0, T_1, x)
= 1 - p(t, T_0, T_1, x)$ then we can readily write equation \eqref{s8e1} in
the form
\begin{equation}\label{s8e2}
\log\left(\frac{q(t,T_0,T_1,x)}{p(t,T_0,T_1,x)}\right) = 
A_1(t+1) + A_2(t+1)(x+t).
\end{equation}
The log-odds of probability of death is thus a linear function of age and the
coefficients of this relation vary with time. The model uses data from 
different cohorts at a common point in time. Therefore, the coefficients 
$A_1(t)$ and $A_2(t)$ are interpreted as factors. The original model of Cairns,
Blake and Dowd used ordinary least square regression to estimate the factors.
The authors plotted the trend of the factors for the period between $1961$ and
$2002$. The factor $A_1(t)$ decreases with time indicating an improvement in
the mortality. On the other hand, $A_2(t)$ is observed to increase with time.
This suggests that the improvements in mortality are felt to a lesser extent 
for older people. In order to forecast the factors at times in future, the 
authors model $A(t) = (A_1(t), A_2(t))^T$ as a random walk with a drift. That
is,
\begin{equation}\label{s8e3}
A(t+1) - A(t) = \mu + CZ(t+1),
\end{equation}
where $\mu$ is a constant vector representing the mean increment, $C$ is a 
constant $2 \times 2$ upper triangular matrix and $Z(t)$ is a standardized
two dimensional Gaussian random variable. The authors also propose a criterion
for a good model of this kind. They introduce the idea of `biological 
reasonability'. In general, a biologically reasonable model will have the 
first component of $\mu$ negative and the second component positive. Further,
the mortality rates for older cohorts should normally be greater than the 
younger cohorts.

Practitioners favor Cairns-Blake-Dowd model for its robustness and the ease
with which it allows inclusion of parameter uncertainty. It is also more 
versatile than the Lee-Carter model because it has an additional factor. 
However, like the Lee-Carter model it does not allow analysis of cohort effects.
Although it models a wide range of advanced years, it does not fit the 
mortality data as well as Lee-Carter model.

\section{Stochastic cohort models}\label{s9}
The stochastic models of Lee-Carter and Cairns-Blake-Dowd were unable to
incorporate cohort effects. To remedy this shortcoming, Renshaw and Haberman
\cite{renshaw2006cohort} proposed a modification of the Lee-Carter model that 
includes cohorts. It can be expressed mathematically as
\begin{equation}\label{s9e1}
\log{M}(x, t) = {a}_x + {b}_x{k}_t + {c}_x\gamma_{t-x},
\end{equation}
where the symbols have the same meaning as in Lee-Carter model. In the new
term, third on the right hand side, $\gamma_{t-x}$ captures the cohort effect.
It is a function of $t - x$, the (approximate) year of birth and ${c}_x$
is just the regression coefficient of this term. Although the model fitted the
male mortality data in England and Wales quite well it turned to not as 
robust as other models. The lack of robustness is believed to be related to
the shape of the likelihood function. It has more than one local maximum.
Therefore, when we change the age or the year range the optimizer ends up
settling on a different extremum each time\cite{cairns2008modelling}. Currie
simplified the Renshaw-Haberman model to
\begin{equation}\label{s9e2}
\log{M}(x, t) = {a}_x + {k}_t + \gamma_{t-x}.
\end{equation}
The simplification solved the problem of robustness but made the fit poorer
\cite{plat2009stochastic}. Plat introduced a model to address the deficiencies
of all the previous models while retaining their benefits 
\cite{plat2009stochastic}. Its mathematical structure is
\begin{equation}\label{s9e3}
\log M(x,t) = a_x + \kappa^1_t + \kappa^2_t(\bar{x} - x) + 
\kappa^3_t(\bar{x} - x)^+ + \gamma_{t-x},
\end{equation}
where $(\bar{x} - x)^+ = \max(\bar{x} - x, 0)$ and $\bar{x}$ is the mean age
in the sample range. The term $a_x$ plays a 
similar role as it did in Lee-Carter model. It controls the general shape
of the mortality curve. Of the four stochastic factors, $\kappa_t^1$
represents the change in the level of mortality across all ages, $\kappa_t^2$
represents the change in the level of mortality between ages, $\kappa_t^3$
captures the differences in mortality behavior in the early adulthood stage
and $\gamma_{t-x}$ captures the cohort effect. If one is interested in 
modeling only the advanced ages then one need not worry about the `accident
hump' of young adulthood and use the simplified model,
\begin{equation}\label{s9e4}
\log M(x,t) = a_x + \kappa^1_t + \kappa^2_t(\bar{x} - x) + \gamma_{t-x},
\end{equation}
Like the Lee-Carter model, this one too suffers from the `identifiability
problem'. For constants $c_1, c_2, d$, the set of parameters
\begin{eqnarray*}
\bar{\gamma}_{t-x} &=& \gamma_{t-x} + c_1 + c_2(t - x) \\
\bar{\kappa}_t^1 &=& \kappa^1_t + c_1 - d\bar{x}c_2 \\
\bar{\kappa}_t^2 &=& \kappa^2_t + dc_2 \\
\bar{a}_x &=& a_x + (1 - d)c_2
\end{eqnarray*}
also lead the the same values for $\log M(x, t)$. The problem is mitigated 
by requiring that 
\begin{eqnarray*}
\sum_{i=x_0}^{x_1} \gamma_i &=& 0 \\
\sum_{i=x_0}^{x_1} i\gamma_i &=& 0 \\
\sum_t\kappa_t^3 &=& 0,
\end{eqnarray*}
where $x_0$ and $x_1$ are the earliest and the latest years of birth to which
the cohort effect is fitted. 

The Plat model is a synthesis of several stochastic mortality models aimed to
retain their advantages and eliminating their shortcomings. Its four stochastic
factors ensure that model is not trivial and yet it is not too complex. It
retains the correlation structure between age groups, making it a valuable
tool for solvency calculations. The model is applicable over the full range of 
human mortality. It captures the cohort effect and has a robust numerical 
behavior. Another valuable feature of the model is that one can develop its
risk-neutral version to price longevity derivatives.

\section{Cause-of-Death models}\label{s11}
All mortality models considered so far have considered only the occurrence of
death without examining the reason. Unlike most of the animal world, death in
humans is not solely due to natural causes. The cause-of-death models split the
aggregate mortality data into a number of causes and model mortality for each
cause separately. It as important to understand the key drivers of human 
mortality as the aggregate mortality rate. It has been observed that deaths
due to cardiovascular diseases have reduced in past few decades. Death rate
due to neoplasm have also begun to drop. However, deaths due to dementia in 
advanced ages and drug-abuse in the younger ages have risen. Features like this
do not emerge by modeling the aggregate death rate. Boumezoued and others
\cite{alexandre2019modeling} used Human Mortality Database and Human Cause-of-
death database to construct a cause-of-death mortality model for the American
population. They define $\mu_{x,t,i}$, the death rate for an age group $x$ due
to cause $i$ in year $t$ as
\begin{equation}\label{s11e1}
\mu_{x,t,i} = \frac{D_{x,t,i}}{E_{x,t}},
\end{equation}
where $D_{x,t,i}$ is the number of deaths by cause $i$ in year $t$ of an 
individual aged $x$ last birthday. $E_{x,t}$ is the total life duration in the
year $t$ of individuals ages $x$ last birthday. It is also called the 
`exposure-to-risk'. Note that the exposure is not indexed by $i$ and is an
aggregate metric. The authors use the competing risks framework according to
which every cause of death has a cause-specific lifetime associated with  it.
Thus if $\tau^A$ is the lifetime under cause $A$ and $\tau^B$ is the lifetime
under all causes except $A$ then the survival function at age $a$ is defined
as the probability that lifetimes under all clauses will be higher than $a$,
Thus,
\begin{equation}\label{s11e2}
S(a) = \pr(\tau > a) = \pr(\tau^A>a,\tau^B>a) = 
\exp\left(-\int_0^a\mu(y)dt\right),
\end{equation}
where
\begin{equation}\label{s11e3}
\tau = \min(\tau^A, \tau^B)
\end{equation}
If the lifetimes under cause $A$ is independent of the lifetime under all causes
then one can write the survival function as
\begin{equation}\label{s11e4}
S(a) = \pr(\tau^A>a)\pr(\tau^B>a)=
\exp\left(-\int_0^a\mu_A(y)dt\right)\exp\left(-\int_0^a\mu_B(y)dt\right),
\end{equation}
In equations \eqref{s11e2} and \eqref{s11e4}, $\mu$ is the aggregate mortality
rate and $\mu_A$ and $\mu_B$ are cause-specific mortality rates. There are 
several ways to build cause of death forecasts. One can associate a link
between the causes and a number of clinical factors and use the latter for
forecasting. However, doing so needs a deep expertise in several medical fields
and the model needs a large amount of medical data. Another way is to use
the multivariate Lee-Carter framework to get the join distribution of cause-
specific death rates. The model is defined by the equation
\begin{equation}\label{s11e5}
\log\mu_{x,t,i} = \alpha_{x,i} + \beta_{x,i}\kappa_{t,i},
\end{equation}
where $\alpha_{x,i}$ is the static, cause-specific age structure, $\kappa_{t,i}$
is the cause-specific mortality and $\beta_{x,i}$ is the sensitivity of the
age class $x$ to the cause $i$. 

Cause-of-death models help us understand the interaction and joint-dynamics
of the causes of death. They allow us to understand the severity of death-
related pathologies. Further, if certain causes of death are prevented by
improvements in healthcare or better policies then one can take that into 
account while building aggregate mortality.

\section{Generalized linear models}\label{s12}
The ordinary least square linear regression assumes that the residuals are
normally distributed. The expected value of the response is a linear
combination of the predictors. Therefore a constant change in the predictor
results in a proportional change in the respose, a characteristic of a linear
relationship. However, often times this is not true. A generalized linear 
model expresses a certain function of the response as a linear model. That is,
instead of fitting a linear model between a response $y$ and the predictors $X$,
we git a linear model between a function $g(y)$ of the response and the
predictors. The function $g$ is called the link function. This method,
developed by Nelder and Wedderburn \cite{nelder1972generalized} proposes a 
general structure for several types of regression including the traditional
ordinary-least-squares, the logistic and the Poisson regressions. 

If the probablity mass or density function of a random variable $Y$ can be
written as
\begin{equation}\label{s12e1}
f_Y(y, \theta; \phi) = 
\exp\left(\frac{y(\theta) - b(\theta)}{\alpha(\phi)} + c(y, \phi)\right)
\end{equation}
where $a, b, c$ are functions of their arguments and $\theta, \phi$ are
parameters then $Y$ is said to belong to an exponential family of distributions.The normal, binomial, Poisson, exponential and gamma distributions are all
members of the exponential family. A generalized linear model assumes that
the responses $Y_1, \ldots, Y_n$ are independent and identically distributed
with the distribution from the exponential family. 

A generalized linear model consists of 1) a distribution from the exponential
family, 2) a linear predictor $\eta = X\beta$ and 3) a link function $g$
such that $\E(Y|X) = g^{-1}(\eta)$.

If we write $B = e^{k_1}$ in equation \eqref{s3e3} describing the force of
mortality of Gompertz' law then we can take logarithm of both sides to get
\begin{equation}\label{e12e2}
\log\mu(x) = k_1 + kx.
\end{equation}
The right hand side of this equation is a linear function in $x$ while the left
hand side is a logarithm. This is precisely the form of generalized linear
model with logarithmic link function and a Poisson distrubution. If the errors
in Gompertz's law are modeled in terms of a binomial distribution then the
link function is a complimentary log-log function \cite{currie2013fitting} 
defined as
\begin{equation}\label{e12e3}
f(x) = \log(-\log(1 - x)),
\end{equation}
where $0 < x < 1$. Mortality laws expressed as sigmoid functions of the form
\begin{equation}\label{e12e4}
q_x = \frac{e^{\theta_0 + \theta_1 x}}{1 + e^{\theta_0 + \theta_1 x}},
\end{equation}
can be easily transformed to the form
\begin{equation}\label{e12e5}
\frac{q_x}{1 - q_x} = e^{\theta_0 + \theta_1 x}
\end{equation}
or, taking logarithm of both sides,
\begin{equation}\label{e12e6}
\log\frac{q_x}{1 - q_x} = \theta_0 + \theta_1 x.
\end{equation}
Once again we observe that the right hand side is a linear function of the
predictor. The function on the left hand is called a logit function and it is
a legitimate link function for a generalized linear model. In fact, this link
function is used to implement the standard logistic regression in applied
statistics. It is possible to show that the Age-Period-Cohort models and the
Cairns-Blake-Dowd models can also be expressed as generalized linear models. 
The algebra involved to demonstrate this fact is a bit involved and we refer
to the paper by Currie \cite{currie2013fitting} for the details.

Generalized linear model gives the theoretical structure to several mortality
models and also suggests a practical implementation. It is important to note
that generalized linear models were not developed to build newer mortality
forecasting models. Rather, deterministic mortality models were cast as 
generalized linear models and that permitted practitioners to use a standard
algorithm to fit their data. Unlike ordinary least squares regression which
has an exact analytical solution, none exists for the generalized linear models.
They must be solved numerically using an optimizer. The optimizer is guaranteed
to find a global minimum if its objective function is convex. When it is not
it is not uncommon for the optimizer to settle on a local minimum. We mentioned
this problem in the case of the Renshaw-Haberman model in section \ref{s9}.

\section{Spline models}\label{s13}
The Lee-Carter model of equation \eqref{s6e1} and the Renshaw-Haberman model
of equation \eqref{s9e1} contain multiplicative terms of predictors. It is 
not possible to cast them as generalized linear models. However, it is possible
to express them as generalized nonlinear models with a link function. Both
these models can be expressed in terms of Poisson distribution and a logarithmic
link function. Like their linear counterparts, generalized non-linear models
do not have an exact analytical solution. Their numerical solution needs an
optimizer. However, it is not always possible to find an optimal solution.
Currie reports that \cite{currie2013fitting} among the $25$ simulations he
tried to fit these models on UK data, in six cases the model did not converge
and in five other cases the optimizer was unable to fit the model. 

Splines were used to draw a smooth curve passing through a set of points. They
were initially used by draftsmen who represented the points by heavy leads and
the splines by flexible wooden strips. This idea was borrowed into numerical
analysis by I. J. Shoenberg \cite{schoenberg1969cardinal} in the late sixties
of the twentieth century. A set of polynomials was used to approximate a
non-linear function where each polynomial fitted a portion of the function
being approximated. It was possible to show that under certian conditions
\cite{schoenberg1969cardinal} a non-linear function could be expressed as a 
piecewise polynomial function. The polynomials were required to blend into each
other that the boundaries in a smooth way. That is, not only were their values
required to match but also their derivatives.

Splines can be implemented using the Cox-de Boor recurrence relation
\cite{de1978practical}
\begin{equation}\label{s13e1}
B_{i,0}(x) = \begin{cases}
1 & \text{ if } x \in [t_i, t_{i+1}] \\
0 & \text{ otherwise}
\end{cases}
\end{equation}
and
\begin{equation}\label{s13e2}
B_{i,j}(x) = \frac{x - t_i}{t_{i+j} - t_i}B_{i,j-1}(x) + 
\frac{t_{i+j+1}-x}{t_{i+j+1}-t_{i+1}}B_{i+1,j-1}(x).
\end{equation}
These splines are then joined at the knot points $\{t_i: i = 1, \ldots, n\}$.
Once the knots are given it is easy to find the splines. However, there is
little guidance on the number of knots. Too few of them lead to under-fitting
and too many of them lead to over-fitting. O'Sullivan \cite{o1986statistical}
suggested that one should use a relatively large number of knots and prevent
over-fitting by levying a penalty on the second derivatives of the splines.
Eilers and Marx \cite{eilers1996flexible} simplify O'Sullivan's suggestion
and apply it to regression on B-splines. They applied the penalty to the
difference between the coefficients of the adjacent splines. The optimal amount
of smoothing is determined by cross-validation and the Akaike Information 
Criterion (AIC). Eilers and Marx call splines with penalties as P-splines.
They demonstrate that P-splines are an extension of the generalized linear
models. P-spline transforms conserve statistical moments of the data and have
polynomial curve fits as limits. Currie and others \cite{currie2004smoothing}
show how to use P-splines to fit mortality forecasting models. If $x_a = (
x_{a,1},\ldots,x_{a,n_a})^T$ and $x_y=(x_{y,1},\ldots,x_{y,n_y})^T$ are the
vectors with ages and years, $\mathcal{B}_a = \{B_{a,1}, \ldots, B_{a, c_a}\}$
and $\mathcal{B}_y = \{B_{y,1},\ldots,B_{y, c_y}\}$ are the cubic B-slines of
dimensions $c_a$ and $c_y$ then
\begin{equation}\label{c14e1}
X = B_y \otimes B_a
\end{equation}
is the regression matrix for the P-spline model
\begin{equation}\label{c14e2}
M(x,t) = \sum_{i,j}\theta_{i,j}B_{i,j}^{ay}(x, t).
\end{equation}
Here $B_a =(B_{a,j}(x_{a,i}))$ is an $n_a \times c_a$ matrix and $B_y = 
(B_{y,j}(x_{y,i}))$ is an $n_y \times c_y$ matrix. The penalty matrix is given
as
\begin{equation}\label{c14e3}
P = \lambda_aI_{c_y} \otimes D_a^TD_a + \lambda_y D_y^TD_y\otimes I_{c_a},
\end{equation}
where $D_a$ is the difference matrix of the rows of M, $D_y$ is the difference
matrix on the columns of M, and the two $\lambda$s are the smoothing parameters.
The regression problem is then minimizing the likelihood function
\begin{equation}\label{c14e4}
\ell_p = \ell - \frac{1}{2} m^TPm,
\end{equation}
where $\ell$ is the default log likelihood function and $m$ is the vector of
the elements of $M$.

The greatest benefit of splines over polynomial regression is that the former is
a more stable numerical method, especially when used for forecasting. The
reason for this stability is the local non-zero support for splines. 
Polynomials, on the other hand, have a global support. The P-spline regression
not only fits the mortality data smoothly, it also produces confidence intervals
to its forecasts enabling us to compute bounds for financial parameters 
depending on mortality.

\section{Conclusion}
We traced the development of mortality models from the earliest attempts of
Graunt and Halley in the seventeenth century to the latest stochastic models
of the twentieth century. In the four centuries spanning these models, we
saw how the models addressed the business needs of their times with the
mathematical methods available to their inventors. The earliest models either
predate or are coeval to the development of Calculus. They were built on the
most rudimentary data. Although their form was crude, their aim was very clear.
They aspired to predict the liabilities of insurance companies. As mathematical
ideas started to get sophisticated, so did the mortality models. By the
eighteenth century, the concept of function was understood my most 
mathematicians at an intuitive level. The idea of probability was still not
within the ambit of mainstream mathematics. As a result, mortality models of
those days were written as functions of human age. The key characteristics
of human mortality did not change drastically enough to warrant anything more
than a more flexible functional form. However, from the closing decades of the
nineteenth century, advances in health sciences and developments of vaccines 
started to prevent many deaths. The developed world also saw a long lull in 
large scale conflict after the second world war got over. The triumph of
medical sciences became a cause of worry to the actuaries. They spotted the
emergence of significant longevity risk to their businesses. They noticed the
rapid rise in human life expectancy leading to more people drawing pensions
for more years. This is when the need to forecast mortality arose. The 
foundations of probability theory were laid in 1930s and stochastic processes
were put on a firm mathematical footing by the 1950s. Actuaries and 
statisticians took advantage of these developments to build stochastic models
to forecast the future course of mortality curves. Their results were not
in the form of a deterministic curve but a probability distribution with
well-defined confidence intervals. The availability of cheap computing also
spurred a growth in this area. Stochastic models began to include cohort 
analysis. With the advent of newer forms of finance and sophisticated
financial engineering, mortality models began to be coupled with design of
financial instruments to hedge risks. This is indeed a very long way since the
publication of 'Bills of Mortality' every Thursday in the parishes of London.
\bibliography{draft}
\bibliographystyle{plain}
\end{document}

